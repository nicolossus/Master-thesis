%================================================================
\chapter{Inference on the HH Model}\label{chap:res_hh}
%================================================================  

In this chapter, we present the results from simulation-based inference on the Hodgkin-Huxley (HH) model's conductance parameters $\gbarK$ and $\gbarNa$. 

%\url{https://github.com/mackelab/sbi/blob/main/examples/00_HH_simulator.ipynb}

%Chapter Inference on the HH model 

%- observed data (clean)
%- priors
%- prior predictive sum stats 
%- rej-abc analysis 
%- rej-abc (original) posteriors 
%- reg adj posteriors
%- ppc 

%- observed data noisy 
%- rej-abc (original) posteriors 
%- reg adj posteriors
%- ppc 

%- sbi

%reproducing AP
%numerical solutions similar
%capture

%================================================================
\section{Observation and Feature Extraction}
%================================================================

Let us assume we current-clamped a neuron and recorded the voltage trace in \autoref{fig:hh_obs_data}. This voltage trace was not actually measured experimentally but synthetically generated by simulating the HH model through the HH simulator in \cw{NeuroModels}. The model was simulated for $T=120\ms$ with step size $\Delta t =10 \ms$ and stimulus $I = 10 \, \mathrm{\mu A/cm}^2$ turned on at $10 \ms$ and off at $110 \ms$. The conductance parameters were set as $\gbarK=36 \gunit$ and $\gbarNa=120 \gunit$. The rest of the HH model's parametrization is given in \autoref{tab:hh_model_parameters}. The idealized voltage trace recording, free of any noise, will be used as the observed data in our first analyses. Hopefully, we can then more easily assess strengths and weaknesses of the algorithms themselves, and not have the results overshadowed by noisy data. Furthermore, the present trace allows us to verify whether the computational implementations are accurate. The ground truth parameters will therefore be the particular values of $\gbarK$ and $\gbarNa$ used in the simulation.  

By visual inspection of the trace in \autoref{fig:hh_obs_data}, the expected shape of an action potential (AP) is reproduced by the numerical solution, which indicates that the implementation of the simulator is accurate. Moreover, since the voltage trace does not display any unexpected abrupt behavior, the time resolution of $\Delta t=0.025 \ms$ seems to be sufficient. 

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.9]{hh_obs_data}
    \caption{Observed voltage trace of a current clamped neuron synthetically generated by the HH simulator simulated for $T = 120 \ms$ with time resolution $\Delta t=0.025 \ms$. The stimulus is a step current $I = 10 \, \mathrm{\mu A/cm}^2$ with onset and offset at $10 \ms$ and $110 \ms$, respectively. Here, the conductance parameters $\gbarK = 36 \gunit$ and $\gbarNa = 120 \gunit$. The present voltage trace being the observation, these conductance parameters are thus the ground truths for the subsequent analyses.}
    \label{fig:hh_obs_data}
\end{figure} 

From the voltage trace, we extract spike statistics by the computational algorithms outlined in \cref{sec:software}. \autoref{fig:hh_stat_extraction} shows the locations in the voltage trace that form the basis of the spike statistic calculations. In fact, the annotations on the voltage trace are set automatically according to the positions found by the extraction algorithms. By the definitions of the different summary statistics provided in \cref{sec:spike_statistics}, we see that the extraction locations are placed correctly on the voltage trace. Consequently, the extraction algorithms seem to function as intended. 

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.9]{hh_stat_extraction}
    \caption{Locations found by the feature extraction algorithms for spike statistic calculations on the observed voltage trace. The locations are annotated by different markers, with labels stated in the legend, which indicate the particular summary statistic calculation they are affiliated with.}
    \label{fig:hh_stat_extraction}
\end{figure} 

\autoref{tab:hh_obs_sumstats} summarizes the calculated summary statistics from the observed voltage trace; (i) \textit{spike rate}, calculated as the number of spikes divided by the duration of the stimulus; (ii) \textit{average AP overshoot}, calculated by averaging the absolute peak voltage of all APs; (iii) \textit{average AP width}, calculated by averaging the width of every AP at the midpoint between its onset and its peak; (iv) \textit{average AHP depth}, calculated by averaging all minima voltage throughs between two consecutive APs; (v) \textit{latency to first spike}, calculated as the time between stimulus onset and first AP peak; (vi) \textit{accommodation index}, which measures the local variance in ISIs and is calculated by \cref{eq:accomm_index}. Comparing the values of the tabulated summary statistics with the information in \autoref{fig:hh_stat_extraction}, we find agreement. There are 7 spikes over the course of the stimulus duration of $100 \ms$, so the spike rate must be $0.07 \, \mathrm{mHz}$. Furthermore, the value of the average AP overshoot and width, as well as the average AHP depth, seem reasonable when compared with the voltage values at the extracted locations. A latency to first spike of about $2 \ms$ also matches what is seen in the voltage trace. There is practically no difference in length between two consecutive ISIs in the voltage trace, and the accommodation index should therefore reflect, as it does, the lack of variability. All in all, this indicates that also the summary statistic calculations are implemented correctly. 

\begin{table}[!htb]
  \caption{Observed voltage trace reduced to a set of summary statistics. See text for details on the statistics.  }
  %\footnotesize%
  \begin{center}
    \rowcolors{2}{gray!15}{white}
    \begin{tabular}{cc}
      \toprule
      \textbf{Summary statistic} & \textbf{Observed value} \\
      \midrule
      %Number of spikes &  7 \\
      Spike rate &  0.0700 mHz \\
      Average AP overshoot & 30.7316 mV  \\
      Average AP width &  2.0501 mV \\
      Average AHP depth & -74.2234 mV \\
      Latency to first spike & 2.3000 ms \\
      Accommodation index &  $2 \cdot 10^{-17}$ \\
      \bottomrule
    \end{tabular}
  \end{center}
  \label{tab:hh_obs_sumstats}
\end{table}

%================================================================
\subsection{Correlation Analysis \& Importance Weights}
%================================================================

Next, we carry out the correlation analysis outlined in \cref{sec:corr_analysis}. The objective of the analysis is to characterize the effects of parameter variability on the output of the model in terms of the summary statistics. The analysis is done by sampling from the prior predictive distribution, and the priors for $\gbarK$ and $\gbarNa$ are shown in \autoref{fig:hh_priors}. 
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.8]{hh_priors}
    \caption{Priors over $\gbarK$ (top) and $\gbarNa$ (bottom). We use both an informative (blue) and noninformative (orange) centered about the ground truth parameter value (red line) for each parameter.}
    \label{fig:hh_priors}
\end{figure}
For each parameter, we use a noninformative prior (orange density) with about $\pm 10\%$ range around the ground truth parameter and a slightly more informative prior (blue density). While technically the informative priors could be classified as weakly-informative (as per the definition given in \cref{sec:coin_flipping}), we will refer to them as informative. 

For each category of priors, we sampled 2000 parameter pairs, fed them to the HH simulator model and calculated the summary statistics from the simulated data for each pair. The spike statistics are only well-defined in the presence of spikes, and accommodation index and average AHP depth need at least two and three spikes, respectively, to be defined. As such, we need to remove samples if they contain ill-defined statistics.

%================================================================
\subsubsection*{With Informative Priors}
%================================================================

Of the 2000 summary statistics samples simulated under the informative priors, 1881 were well-defined. Scatter plots for a subset of these are shown in \autoref{fig:hh_priorpred_sstats_normal}, where the summary statistics are shown as functions of the pairs of parameter values. Thus, the scatter plots enable us to see the variability the different summary statistics exhibit relative to change in model parameter values. Each point indicates the relative magnitude of the statistic by its size and color, with a reference table stated in the legend along with the name of the particular statistic.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{hh_priorpred_sstats_normal}
    \caption{Scatter plots of summary statistics simulated with different pairs of model parameter values. The summary statistics were simulated under the joint informative prior predictive distribution. Of 2000 generated samples, 1881 were well-defined. Here, a subset of 470 samples are shown. Each summary statistic is assigned to its own panel, with the particular statistic stated in the legend. Each point represents the value of a summary statistic for a pair of parameter values, $\qty(\gbarK, \gbarNa)$. The color of a point indicates the relative magnitude of the statistic, for which bright colors represent small and dark colors large values, also indicated in each subplot legend. The scatter plots thus indicate the variability of summary statistics relative to movement of the pairs of model parameter values.
    }
    \label{fig:hh_priorpred_sstats_normal}
\end{figure} 
In addition to displaying the variability, the scatter plots also indicate if there is a systematic relationship between a parameter and summary statistic. By fixing the value of one of the parameters and following its line-of-sight, we can assess whether the other parameter systematically increments or decrements a summary statistic. Average AP overshoot exhibits a steady variability and seems to follow an approximately linear trend. This also applies to the average AP width, though to a lesser extent. The approximate linear relationship is most apparent for $\gbarK$ in both cases. In terms of the underlying biophysical mechanisms, we would expect average AP overshoot to be most sensitive to $\gbarNa$ and average AP width to $\gbarK$. The role of the \Na channel in AP generation is well-established, and AP overshoot is tied to the fast \Na channel dynamics. Likewise, given the role of the \K channel in repolarizing the neuron after an AP, the average AP width is tied to the duration of the recovery period. While these expectations are not clearly present in the scatter plots, we should keep in mind that we only explore a fairly limited region of the parameter space. We should therefore be cautious in our interpretation of how well the scatter plots indicate sensitivity. Accommodation index has almost no variation, which might be unsurprising since it measures the local variance in ISIs. The constant current protocol does not facilitate much variation in spike trains. Thus, for the observed voltage trace at hand, accommodation index is not an informative summary statistic. However, it could be useful in characterizing spike trains generated under more complex current protocols. Average AHP depth also exhibits little variation, whereas latency to first spike an ample amount. Though, for latency to first spike there is no apparent systematic relationship, as it increments and decrements right and left. This might not be optimal for constraining the model parameters. Finally, the spike rate, though it does not vary much, seem to have a more systematic and stable relationship with the model parameters, predominantly $\gbarK$. It behaves more like a step function, where it retains a particular value for a prolonged range of $\gbarK$ values. In conclusion, how well this set of summary statistics will constrain the model parameters needs to be investigated further down the line.  

In order to not rely solely on visual inspection of the sensitivity, \autoref{fig:hh_weights_normal} provides the pairwise Pearson's correlation coefficients of each model parameter and summary statistics, as well as the importance weights derived from the correlation coefficients.
% subfigure
\begin{figure}[!htb]
\centering
\subfloat[]{{\includegraphics[scale=0.65]{hh_priorpred_corr_normal}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.65]{hh_priorpred_weights_normal}}}
\caption{\textbf{(a)} The pairwise Pearson's correlation coefficients for the model parameters and summary statistics. \textbf{(b)} The importance weights calculated from the correlation coefficients, see \cref{sec:corr_analysis} for details. Note that the weights sum to one.
}
\label{fig:hh_weights_normal}
\end{figure}
As was indicated by the scatter plots, $\gbarK$ has a stronger (approximately) linear relationship with the summary statistics than $\gbarNa$. The interpretation of these results are that the summary statistics related to the shape of an AP encode the most information, with average AP overshoot being the most dominant. Moreover, the results indicate that $\gbarK$ will be constrained better than $\gbarNa$ by these summaries when performing regression adjustment. Since the relationship between $\gbarK$ and the summary statistics is much stronger than for $\gbarNa$, the weighting scheme also becomes biased towards $\gbarK$. The weighting is therefore a bit unfair for $\gbarNa$. For instance, even though the spike rate is weakly correlated with $\gbarNa$, it receives a heavy weight because it is strongly correlated with $\gbarK$. The weight of the spike rate even surpasses that of average AP width, which shows a decent amount of correlation with both $\gbarNa$ and $\gbarK$. The pairwise Pearson's correlation coefficients strong assumption about linearity again necessitates the need to be wary of the interpretation of which summary statistics are the most informative. As the weighting scheme prefers $\gbarK$, we should also investigate the effects importance weights have on the inference further down the line.

%================================================================
\subsubsection*{With Noninformative Priors}
%================================================================

The findings with noninformative priors are similar to the ones discussed above. This is perhaps unsurprising, since a prior does not alter the intrinsic relationship between a parameter and summary statistic, just how the samples are distributed in the parameter space. The corresponding figures with samples from the joint noninformative prior distribution can be found in \cref{sec:Appendix A}.


%================================================================
\section{Study of ABC Settings}
%================================================================

We now turn to a study concerning the tuning parameters in the rejection ABC algorithm. As the generation of one posterior amounts to a single stochastic trial, we will generate several posteriors for the same settings in order to assess potential variability in the results. Here, we use the rejection sampler in \cw{pyLFI} to infer the conductance parameters in the HH model. The performance metrics RMSPE and SEM, defined in \cref{sec:performance_metrics}, will be used to assess the accuracy and variability, respectively, of a particular inference setting.

In our first tuning parameter analysis, we study the effect of the tolerance parameter $\epsilon$ in terms of the $p_\epsilon$-quantile of the distances. For each quantile, we generate 10 posterior with 1000 posterior samples in each. As discussed in \cref{sec:software}, this means that the tolerance will be set via a pilot study. For each quantile, the pilot study performs 2000 simulations to estimate both the tolerance and the scale of the summary statistics. In this analysis, the summary statistics are equally weighted. Having obtained a posterior, we perform local linear regression adjustment with the Epanechnikov kernel and log transformation of the parameters to obtain a corresponding adjusted posterior. We do the above using both the informative and noninformative priors. \autoref{fig:RMSPE_vs_quantile} shows the results.
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.8]{RMSPE_vs_quantile}
    \caption{The RMSPE in posteriors over $\gbarK$ (top) and $\gbarNa$ (bottom) against the $p_\epsilon$-quantile as a measure of tolerance. Each point is the mean RMSPE over 10 posteriors, each consisting of 1000 posterior samples, and the SEM is shown as a vertical bar. The posteriors were generated by the rejection ABC algorithm and then adjusted with local linear regression adjustment. Whether an estimate of error had informative/noninformative priors or is from the original/adjusted posterior is color coded (see legend).}
    \label{fig:RMSPE_vs_quantile}
\end{figure} 
RMSPE measures the percentage difference between the ground truth and a weighted estimate that accounts for the width of the posterior. Increasing $p_\epsilon$-quantiles amount to accepting simulated data that are increasingly further away from the observed data. As such, we expect the error to increase with $p_\epsilon$, due to more distorted approximations. This is also the general trend, though the differences in error between posteriors are generally small. Here, the posterior error estimates of $\gbarK$ and $\gbarNa$ differ. Focusing on the estimates of error in the original posterior samples, i.e., the samples obtained solely with the rejection ABC algorithm, $\gbarK$ has larger errors than $\gbarNa$. The errors of $\gbarNa$ remain almost constant as $p_\epsilon$ increases, and are actually slightly larger for the lowest $p_\epsilon$. The reason for this may be intricate, but most likely it has to do with correlation between the posterior samples of $\gbarNa$ and $\gbarK$ and that the simulated summary statistics accepted under a strict tolerance happen to shift the $\gbarNa$ estimate for the worse (by a little amount) and $\gbarK$ for the better. In terms of variation, the SEM of all the mean RMSPE shows that the inferred posteriors for the same settings are practically indistinguishable. We also see that estimates with informative priors converge better than those with noninformative priors, as expected. The RMSPE of the regression adjusted posterior estimates are significantly more accurate than the original posterior estimates. The improvement is most prominent for $\gbarK$, and aligns with the expectation obtained from the correlation analysis; since $\gbarK$ has a stronger linear relationship with the summary statistics than $\gbarNa$, the local linear regression model will give better adjustment of the $\gbarK$ posterior samples. The difference in error when $p_\epsilon$ increases are tiny for $\gbarK$, which suggests that the regression approach manages to correct the $\gbarK$ samples as if they were sampled from $\pi_\mathrm{ABC} \qty(\gbarK \mid \rho \qty(\ssim, \sobs) \leq \epsilon)$ with $\epsilon=0$. For $\gbarNa$, however, this breaks down and the error increases with $p_\epsilon$, with a particular jump between the 0.1 and 0.5-quantiles. Nevertheless, with regression adjustment, more simulations can be accepted without sacrificing substantial accuracy, especially for the model parameters that are the most constrained by the summary statistics. In addition, the difference in error between using informative and noninformative priors is also reduced when performing regression adjustment. 

Next, we investigate error in the estimates against the number of summary statistics used to constrain the model parameters. We start with only a single statistic, average AP overshoot, and increment by one more according to the following order; spike rate, average AP width, average AHP depth, latency to first spike and accommodation index. Again, we use the quantile-based rejection ABC algorithm with local linear regression adjustment using the Epanechnikov kernel to estimate the posteriors. We use the 0.4-quantile as a compromise between accuracy and run time (see \autoref{fig:runtime}). From the preceding result (\autoref{fig:RMSPE_vs_quantile}) we found that the posteriors generated for the same settings are practically identical. Here, we therefore only generate a single posterior for each number of summary statistics. This is, however, done for both the informative and noninformative priors. In addition, we generate posteriors for both cases of weighting of the summary statistics; either equally or importance weighted (we ensure the importance weights sum to one). The results are shown in \autoref{fig:RMSPE_vs_n_sumstats}.
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.8]{RMSPE_vs_n_sumstats}
    \caption{The RMSPE in posteriors over $\gbarK$ (top) and $\gbarNa$ (bottom) against the number of summary statistics. The first, single statistic is (i) average AP overshoot, and then the number of statistics is incremented by one more according to the following order; (ii) spike rate, (iii) average AP width, (iv) average AHP depth, (v) latency to first spike and (vi) accommodation index. Each point is the RMSPE in a regression adjusted posterior consisting of 1000 posterior samples. Whether an estimate of error had informative/noninformative priors or equally/importance weighted the summary statistics is color coded (see legend).
    }
    \label{fig:RMSPE_vs_n_sumstats}
\end{figure} 
The figure shows that the HH model is more tightly constrained by increasing the number of summary statistics, particularly $\gbarK$ as we have already discussed. Though, if we use summary statistics that do not capture relevant information for the parameters, it might lead to worse inference. The set of summary statistics that gives the most accurate posteriors consists of: (i) average AP overshoot, (ii) spike rate, (iii) average AP width and (iv) average AHP depth. For $\gbarK$, inclusion of the remaining two statistics, (v) latency to first spike and (vi) accommodation index, does not lead to a noticeable difference in accuracy. Neither does using equally or importance weighted summary statistics. For $\gbarNa$, on the other hand, the error becomes moderately worse by including (v) latency to first spike and (vi) accommodation index. Here, using importance weights actually helps to constrain $\gbarNa$ and improves the error in the posterior. 


%================================================================
\section{Summarizing Posteriors}
%================================================================

We can assess the identifiability of the HH model's active conductance parameters by examining the locations and widths of the resulting posterior estimates. A wide, flat posterior on a parameter indicates a large number of equally optimal values, which suggests that the parameter may be unidentifiable. As outlined in \cref{sec:performance_metrics}, the goodness of fit of the inferred posteriors will be considered through the MAP estimate, $95\%$ highest density interval (HDI) and, since we have access to the ground truths, RMSPE. We will also use posterior predictive checks (PPCs) to check for auto-consistency. To reiterate the settings of the rejection ABC sampler; we generate the following posteriors using the 0.4-quantile to determine the tolerance and the set of importance weighted summary statistics (i) average AP overshoot, (ii) spike rate, (iii) average AP width and (iv) average AHP depth.

Informed by the preceding findings, going forward we will use the set of summary statistics labelled (i)-(iv) above and also keep the inclusion of importance weights. 

%================================================================
\subsection{Posteriors from Informative Priors}
%================================================================

\autoref{fig:hh_posterior_org_normal} shows the original posteriors over $\gbarK$ and $\gbarNa$, with summarizing metrics stated in the legends. Compared to the informative priors over the model parameters (see \autoref{fig:hh_priors}), the updated state of knowledge represented by the posteriors is more constrained. The MAP estimates are centered close on the ground truth parameters, which, by the definition of MAP, means that the ground truth parameters are in regions of high posterior density. Compared to the $\gbarNa$ posterior, the RMSPE of the $\gbarK$ posterior is slightly larger, even though the 95\% HDI of $\gbarK$ is narrower than that of $\gbarNa$. This might be a bit surprising, but can be explained by noticing the sharpness of the posterior peaks. The peak of the $\gbarK$ posterior is more flat compared to the peak of the $\gbarNa$ posterior, which is quite sharp about the ground truth, and this is reflected in the RMSPE measure.
\begin{figure}[H]
    \centering
    \includegraphics[scale=1.0]{hh_posterior_org_normal}
    \caption{Original rejection ABC posteriors over the Hodgkin-Huxley model parameters $\gbarK$ (top) and $\gbarNa$ (bottom). Here, the parameter proposals were sampled from the joint informative prior distribution. The dark shaded region indicates the 95\% HDI and the dotted line the MAP estimate. The ground truth is indicated by the red line. The legend states the numerical values for each of these, in addition to the RMSPE in the posterior.}
    \label{fig:hh_posterior_org_normal}
\end{figure}

\autoref{fig:hh_joint} maps the correlation, in terms of pairwise Pearson's correlation coefficients, between the posterior samples of $\gbarK$ and $\gbarNa$. It shows that the parameter samples are indeed strongly correlated. Consequently, for predictive sampling from the posteriors, we need to sample from the \textit{joint posterior}, also shown in the figure.
% subfigure
\begin{figure}[!htb]
\centering
\subfloat[]{{\includegraphics[scale=0.5]{hh_corr_org_normal}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.65]{hh_joint_posterior_org_normal}}}
\caption{\textbf{(a)} The pairwise Pearson's correlation coefficients for the posterior samples of $\gbarK$ and $\gbarNa$. \textbf{(b)} The joint posterior distribution of $\gbarK$ and $\gbarNa$. Darker regions correspond to higher density, and the ground truth is indicated by the red marker and axis lines. Since the marginal posteriors over model parameters (shown on the marginal axes) are highly correlated, predictive posterior samples need to be sampled from the joint posterior. 
}
\label{fig:hh_joint}
\end{figure}

\autoref{fig:hh_posterior_reg_normal} shows the regression adjusted posterior. Here, $\gbarK$ is constrained to the interval $[35.986, 36.029]$ with 95\% probability, and $\gbarNa$ to the $[120.001, 120.423]$ with 95\% probability. The RMSPE in both posteriors are consequently reduced significantly compared with RMSPE in the original posteriors (\autoref{fig:hh_posterior_org_normal}). Again, we see that the corrected $\gbarK$ samples become more constrained than the $\gbarNa$ samples, due to the stronger linear relationship $\gbarK$ has with the summary statistics. The regression adjustment overshoots the ground truth of $\gbarNa$ by a tiny amount, but the estimated parameter range could be equally capable of describing the observed data. The regression adjusted posteriors are able to identify the conductance parameters remarkably well, which is promising for using ABC algorithms to identify parameters in other conductance-based neural models based on the HH formalism as well. 
\begin{figure}[H]
    \centering
    \includegraphics[scale=1.0]{hh_posterior_reg_normal}
    \caption{Regression adjusted rejection ABC posteriors over the Hodgkin-Huxley model parameters $\gbarK$ (top) and $\gbarNa$ (bottom). Here, the parameter proposals were sampled from the joint informative prior distribution. The dark shaded region indicates the 95\% HDI and the dotted line the MAP estimate. The ground truth is indicated by the red line. The legend states the numerical values for each of these, in addition to the RMSPE in the posterior.}
    \label{fig:hh_posterior_reg_normal}
\end{figure}

To verify that the parameter ranges in the regression adjusted posteriors are able to describe the observed data well, we perform a PPC. We draw 100 samples from the joint posterior predictive distribution and feed the parameters to the HH simulator. We then take the average of the simulated voltage traces. The result can be seen in \autoref{fig:hh_postpred_reg_normal}, where we plot the predicted simulations and their mean together with the observed voltage trace. 
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=1.0]{hh_postpred_reg_normal}
    \caption{Graphical posterior predictive check comparing the observed voltage trace to simulated data predicted by the Hodgkin-Huxley model under the regression adjusted joint posterior predictive distribution.}
    \label{fig:hh_postpred_reg_normal}
\end{figure}
As can be seen from the figure, the samples from the inferred joint posterior lead to simulations that are virtually identical to the observed data, confirming that the procedure succeeds at capturing the observed data and identifying the underlying parameters.


%================================================================
\subsection{Posteriors from Noninformative Priors}
%================================================================

By now, it has been demonstrated that regression adjustment of the posteriors is crucial for improving the accuracy of estimates from the rejection ABC sampler. \autoref{fig:hh_posterior_reg_uniform} shows the regression adjusted posteriors over $\gbarK$ and $\gbarNa$ when we use noninformative priors.
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=1.0]{hh_posterior_reg_uniform}
    \caption{Regression adjusted rejection ABC posteriors over the Hodgkin-Huxley model parameters $\gbarK$ (top) and $\gbarNa$ (bottom). Here, the parameter proposals were sampled from the joint noninformative prior distribution. The dark shaded region indicates the 95\% HDI and the dotted line the MAP estimate. The ground truth is indicated by the red line. The legend states the numerical values for each of these, in addition to the RMSPE in the posterior.}
    \label{fig:hh_posterior_reg_uniform}
\end{figure} 
Compared with the regression adjusted posteriors where we used informative priors, the present posteriors are only marginally less accurate. This means that, even with data-driven ABC inference, the HH conductance parameters can be accurately identified.


%================================================================
\section{SNPE Posteriors}
%================================================================

Rejection ABC is one of simplest simulation-based inference algorithms. We have added certain refinements to the standard rejection ABC sampling procedure, such as regression adjustment, weighted Euclidean distance and quantile-based rejection. Now, we will compare the posteriors obtained through our implementation of ABC in \cw{pyLFI} to one of the more recent advancements in the field; neural density estimation (NDE). In particular, the NDE algorithm SNPE introduced in \cref{sec:nde}. The objective here is not to perform an exhaustive analysis of SNPE and its capabilities, as this is demonstrated in the original papers \cite{SNL_first}, \cite{SNPE_first} and \cite{SNPE_apt}. Here, and in subsequent analyses with SNPE, the aim is to compare how its estimated posteriors compare with the ABC posterior under similar settings. This means that we will train the neural density estimator to learn the association between summary statistics of the data and the underlying parameters. As neural density estimator we use a particular normalizing flow (NF) called \textit{masked autoregressive flow} (MAF) that is developed by Papamakarios et al. \cite{MAF}. SNPE is given a modified HH simulator, which returns the same set of summary statistics we used in the preceding analyses; (i) average AP overshoot, (ii) spike rate, (iii) average AP width and (iv) average AHP depth. Moreover, we use the same noninformative priors as in \autoref{fig:hh_priors}, and train the network on 1000 simulations. The resulting posteriors over $\gbarK$ and $\gbarNa$ are shown in \autoref{fig:hh_post_sbi}.
\begin{figure}[H]
    \centering
    \includegraphics[scale=1.0]{hh_post_sbi}
    \caption{SNPE posteriors over the Hodgkin-Huxley model parameters $\gbarK$ (top) and $\gbarNa$ (bottom). The dark shaded region indicates the 95\% HDI and the dotted line the MAP estimate. The ground truth is indicated by the red line. The legend states the numerical values for each of these, in addition to the RMSPE in the posterior.}
    \label{fig:hh_post_sbi}
\end{figure}
Both posteriors we obtain are narrow and sharply peaked about the ground truth parameters, meaning that SNPE is able to identify admissible parameters as well. The summaries of the estimated posteriors are similar to the ones for the regression adjusted ABC posteriors. Though it is a close competition, the ABC posteriors are actually slightly more narrow than the SNPE posteriors. Moreover, the outliers observed in the tails of the SNPE posterior are not present in the ABC posteriors. This comparison might not be entirely fair to SNPE, as it is difficult to pin-point exactly how many training simulations that measure up to be “under similar settings”. By training the network on even more simulations, the posteriors could perhaps be made even more narrow. Furthermore, it is likely that the parameter ranges in the SNPE posteriors all are compatible with the observed data, which will be illuminated in more detail in the next section


%PPC
%\begin{figure}[H]
%    \centering
%    \includegraphics[scale=0.8]{hh_post_pred_sbi}
%    \caption{caption}
%    \label{fig:fig1}
%\end{figure}

%see that the wider posterior has an effect on recreating the observed data, this is not present in rej-abc reg adjust posterior due to it being more narrow


%================================================================
\section{Noisy Observation}
%================================================================

So far we have only used an idealized voltage trace, free of any noise, as the observed data. However, real-world neural data are quite noisy. Here, we will examine the impact a noisy observed recording has on the inference with the rejection ABC sampler. There are several sources to noise in cellular dynamics, and, in extension, ways to introduce noise to the Hodgkin-Huxley equations, see e.g. \cite{hh_noise} for a review. We will introduce noise to the observed voltage trace by using a stochastic version of the HH model that incorporates current noise as a Gaussian white noise process. Besides the inclusion of current noise, we use the same same settings for the HH simulator as earlier and record the voltage trace seen in \autoref{fig:hh_noisy_data}.
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.9]{hh_noisy_data}
    \caption{Noisy voltage trace generated with the HH simulator by introducing Gaussian white noise to the input stimulus. The parametrization of the HH model and simulation parameters are identical to the ones used in preceding noise-free voltage trace (\autoref{fig:hh_obs_data}).}
    \label{fig:hh_noisy_data}
\end{figure} 
The corresponding summary statistics are tabulated in \autoref{tab:hh_noisy_sumstats}. 
\begin{table}[!htb]
  \caption{Summary statistics extracted from the noisy observed voltage trace.}
  %\footnotesize%
  \begin{center}
    \rowcolors{2}{gray!15}{white}
    \begin{tabular}{cc}
      \toprule
      \textbf{Summary statistic} & \textbf{Observed value} \\
      \midrule
      Number of spikes &  7 \\
      Spike rate &  0.0700 mHz \\
      Average AP overshoot & 30.7223 mV  \\
      Average AP width & 2.0679 mV \\
      Average AHP depth & -74.3394 mV \\
      Latency to first spike & 2.2750 ms \\
      Accommodation index &  -0.0067 \\
      \bottomrule
    \end{tabular}
  \end{center}
  \label{tab:hh_noisy_sumstats}
\end{table}

We then use the rejection ABC sampler in \cw{pyLFI} and a HH simulator that generates noise-free simulations to infer the underlying parameters in the observed voltage trace. Though our observation is not a particularly noisy recording, it still distorts the regression adjusted ABC posteriors significantly compared to when we used noise-free observed data, as seen in \autoref{fig:hh_posterior_reg_noisy}.
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=1.0]{hh_posterior_reg_noisy}
    \caption{Regression adjusted rejection ABC posteriors over the Hodgkin-Huxley model parameters $\gbarK$ (top) and $\gbarNa$ (bottom) with noisy observed voltage trace. Here, the parameter proposals were sampled from the joint informative prior distribution. The dark shaded region indicates the 95\% HDI and the dotted line the MAP estimate. The ground truth is indicated by the red line. The legend states the numerical values for each of these, in addition to the RMSPE in the posterior.}
    \label{fig:hh_posterior_reg_noisy}
\end{figure} 
As can be seen, the ground truth parameters are no longer in regions of high posterior density -- they are not actually included in the posteriors at all. Though both the $\gbarK$ and $\gbarNa$ posteriors are narrow and sharp, their locations in parameter space are shifted toward a completely different set of parameter values, especially $\gbarNa$, than we found with the noise-free observation. However, if do a graphical PPC, as seen in \autoref{fig:hh_postpred_reg_noisy}, we find that the simulations predicted by the joint posterior match the observed voltage trace surprisingly well. 
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=1.0]{hh_postpred_reg_noisy}
    \caption{Graphical posterior predictive check comparing the noisy observed voltage trace to simulated data predicted by the Hodgkin-Huxley model under the regression adjusted joint posterior predictive distribution. The posterior predictive mean is the average of 100 predicted simulations.}
    \label{fig:hh_postpred_reg_noisy}
\end{figure}
This example bolster the motivation for why the Bayesian approach to inference should be considered; there might be multiple parameter settings that are consistent with the observed data.




%===============================================================
%===============================================================
%===============================================================
%===============================================================
%===============================================================

%================================================================
\chapter{Inference on the Brunel Model}\label{chap:res_brunel}
%================================================================ 

In this chapter, we present the results from simulation-based inference on the Hodgkin-Huxley (HH) model's conductance parameters $\gbarK$ and $\gbarNa$. 


Chapter Inference on the Brunel model
- observed data 
- priors
- prior predictive sum stats 
- rej-abc (original) posteriors 
- reg adj posteriors
- ppc 
- sbi


%%
%%
%%
%%
%%

%================================================================
%\chapter{Analysis of the Hodgkin-Huxley Model}
\chapter{Analysis of the Neuroscientific Models}
%================================================================

lat to first spike, accomm index -> unsurprising, little variation, could perhaps be more useful to characterize spike trains generated under more complex current protocols. 




\textit{accommodation index}, which measures the local variance in ISIs and is calculated by

% chapter: Inference on the Hodgkin-Huxley Model 
% chapter: Inference on the Brunel Model





%================================================================
\section{The Brunel Network Model}
%================================================================

create the observed data

summary statistics of observation 

summary statistics from prior predictive

weights


%================================================================
\chapter{Parameter Identification with REJ-ABC}
%================================================================

%================================================================
\section{Rejection ABC Posteriors on Conductance Parameters}
%================================================================

show how Hodgkin–Huxley model is more tightly constrained by increasing numbers of data features

We also inferred HH parameters for 8 in vitro recordings from the Allen Cell Types database using the same current-clamp stimulation protocol as in our model [60, 70] (Fig. 4F, Supplementary Fig. 8). In each case, simulations based on the SNPE-inferred posterior closely resembled the original data (Fig. 4F). We note that while inferred parameters differed across recordings, some parameters (the spike threshold, the density of sodium channels, the membrane reversal potential and the density of potassium channels) were consistently more strongly constrained than others (the intrinsic neural noise, the adaptation time constant, the density of slow voltage-dependent channels and the leak conductance) (Supplementary Fig. 8). Overall, these results suggest that the electrophysiological responses measured by this current-clamp protocol can be approximated by a single-compartment HH model, and that SNPE can identify the admissible parameters.



%================================================================
%\section{ABC Settings for Identification of Brunel Network Parameters}
%================================================================

%================================================================
%\section{Rejection ABC Posteriors on Synaptic Weight Parameters}
%================================================================

%================================================================
\chapter{Brunel ABC Results}
%================================================================

Brunel 

\section{Observation}

observed spiketrain 

\begin{figure}[H]
    \centering
    \includegraphics[scale=1.0]{brunel_ai_observation}
    \caption{caption}
    \label{fig:fig1}
\end{figure}

sum stats

% Alternating row colors
\begin{table}[H]
  \caption{AI state observed sum stats}
  %\footnotesize%
  \begin{center}
    \rowcolors{2}{gray!15}{white}
    \begin{tabular}{cc}
      \toprule
      \textbf{Summary statistic} & \textbf{Observed value} \\
      \midrule
      Mean firing rate &  0.0366 kHz \\
      Mean CV &  0.4250  \\
      Fano factor & 0.2341  \\
      \bottomrule
    \end{tabular}
  \end{center}
  \label{tab:hh_noisy_sumstats}
\end{table}

correlation (pearson) coefficient matrix

\begin{figure}[H]
    \centering
    \includegraphics[scale=1.0]{brunel_obs_corr}
    \caption{caption}
    \label{fig:fig1}
\end{figure}

\section{prior pred}

priors

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{brunel_priors}
    \caption{caption}
    \label{fig:fig1}
\end{figure}

sum stats scatter (500 samples)

\begin{figure}[H]
    \centering
    \includegraphics[scale=1.0]{brunel_sum_stats}
    \caption{caption}
    \label{fig:fig1}
\end{figure}


sum stats correlation and weights

% subfigure
\begin{figure}[H]
\centering
\subfloat[]{{\includegraphics[scale=0.7]{brunel_sum_stats_corr}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.7]{brunel_sum_stats_weights}}}
\caption{\textbf{(a)} sum stats correlation. \textbf{(b)} sum stats weights
}
\label{fig:fig1}
\end{figure}

corr coefs of these particular summary statistics indicate that the AI state is most sensitive to the relative strength of inhibitory synapses $g$. Thus, we expect the summary statistics to constrain the $g$ parameter better.


\section{Posteriors} 

Ran 2000 simulations of the model with different parameters drawn from the priors. RMSPE vs quantile: 

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{brunel_quantile_rmspe}
    \caption{caption}
    \label{fig:fig1}
\end{figure}

Use 0.3-quantile, compromise between accuracy and number of samples in the posterior

original posterior

\begin{figure}[H]
    \centering
    \includegraphics[scale=1.0]{brunel_posterior_org}
    \caption{caption}
    \label{fig:fig1}
\end{figure}

reg adjusted posterior

\begin{figure}[H]
    \centering
    \includegraphics[scale=1.0]{brunel_posterior_reg}
    \caption{caption}
    \label{fig:fig1}
\end{figure}


joint posterior (reg adjusted)

\begin{figure}[H]
    \centering
    \includegraphics[scale=1.0]{brunel_joint_posterior_reg}
    \caption{caption}
    \label{fig:fig1}
\end{figure}

posterior predictive checks with reg adjusted samples, 50 samples from posterior pred

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{brunel_post_pred}
    \caption{caption}
    \label{fig:fig1}
\end{figure}

The pairwise Pearson's correlation coefficient is a measure of how synchronous the spiking of a network is. This correlation coefficient measures the correlation between the spike trains of two neurons in the network. In Figure X we examine how this correlation depends on parameter uncertainties by plotting the mean and standard deviation for the pairwise Pearson's correlation coefficient in the AI state.

% subfigure
\begin{figure}[H]
\centering
\subfloat[]{{\includegraphics[scale=0.6]{brunel_pred_corr}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.6]{brunel_pred_corr_std}}}
\caption{\textbf{(a)} mean correlation coefficient matrix. \textbf{(b)} standard deviation
}
\label{fig:fig1}
\end{figure}





%================================================================
\section{Brunel SBI Results}
%================================================================

%================================================================
\subsection{AI state} 
%================================================================



0.037222	0.401465	0.145315

Posterior:

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{brunel_post_ai_sbi}
    \caption{caption}
    \label{fig:fig1}
\end{figure}

ppc 

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{brunel_post_pred_ai_sbi}
    \caption{caption}
    \label{fig:fig1}
\end{figure}

corr 

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{brunel_pred_corr_sbi_ai}
    \caption{caption}
    \label{fig:fig1}
\end{figure}

corr std

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{brunel_pred_corr_std_ai_sbi}
    \caption{caption}
    \label{fig:fig1}
\end{figure}

%================================================================
\subsection{SR state}
%================================================================

SR observation

\begin{figure}[H]
    \centering
    \includegraphics[scale=1.0]{brunel_sr_observation}
    \caption{caption}
    \label{fig:fig1}
\end{figure}

Recorded spike trains from the Brunel network and observed the following summary statistics:

\begin{table}[H]
  \caption{SR state observation.}
  %\footnotesize%
  \begin{center}
    \rowcolors{2}{gray!15}{white}
    \begin{tabular}{cc}
      \toprule
      \textbf{Summary statistic} & \textbf{Observed value} \\
      \midrule
      Mean firing rate &  0.3333 kHz \\
      Mean CV &  0.0121  \\
      Fano factor & 0.007  \\
      \bottomrule
    \end{tabular}
  \end{center}
  \label{tab:hh_noisy_sumstats}
\end{table}

Posterior:

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{brunel_post_sr_sbi}
    \caption{caption}
    \label{fig:fig1}
\end{figure}

ppc 

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{brunel_post_pred_sr_sbi}
    \caption{caption}
    \label{fig:fig1}
\end{figure}

corr 

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{brunel_pred_corr_sbi_sr}
    \caption{caption}
    \label{fig:fig1}
\end{figure}

corr std

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{brunel_pred_corr_std_sr_sbi}
    \caption{caption}
    \label{fig:fig1}
\end{figure}
