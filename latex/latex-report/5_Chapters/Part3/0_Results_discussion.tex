%================================================================
\chapter{Inference on the HH Model}\label{chap:res_hh}
%================================================================  

In this chapter, we present the results from simulation-based inference on the Hodgkin-Huxley (HH) model's conductance parameters $\gbarK$ and $\gbarNa$. 

%\url{https://github.com/mackelab/sbi/blob/main/examples/00_HH_simulator.ipynb}

%Chapter Inference on the HH model 

%- observed data (clean)
%- priors
%- prior predictive sum stats 
%- rej-abc analysis 
%- rej-abc (original) posteriors 
%- reg adj posteriors
%- ppc 

%- observed data noisy 
%- rej-abc (original) posteriors 
%- reg adj posteriors
%- ppc 

%- sbi

%reproducing AP
%numerical solutions similar
%capture

%================================================================
\section{Observation and Feature Extraction}
%================================================================

Let us assume we current-clamped a neuron and recorded the voltage trace in \autoref{fig:hh_obs_data}. This voltage trace was not actually measured experimentally but synthetically generated by simulating the HH model through the HH simulator in \cw{NeuroModels}. The model was simulated for $T=120\ms$ with step size $\Delta t =10 \ms$ and stimulus $I = 10 \, \mathrm{\mu A/cm}^2$ turned on at $10 \ms$ and off at $110 \ms$. The conductance parameters were set as $\gbarK=36 \gunit$ and $\gbarNa=120 \gunit$. The rest of the HH model's parametrization is given in \autoref{tab:hh_model_parameters}. The idealized voltage trace recording, free of any noise, will be used as the observed data in our first analyses. Hopefully, we can then more easily assess strengths and weaknesses of the algorithms themselves, and not have the results overshadowed by noisy data. Furthermore, the present trace allows us to verify whether the computational implementations are accurate. The ground truth parameters will therefore be the particular values of $\gbarK$ and $\gbarNa$ used in the simulation.  

By visual inspection of the trace in \autoref{fig:hh_obs_data}, the expected shape of an action potential (AP) is reproduced by the numerical solution, which indicates that the implementation of the simulator is accurate. Moreover, since the voltage trace does not display any unexpected abrupt behavior, the time resolution of $\Delta t=0.025 \ms$ seems to be sufficient. 

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.9]{hh_obs_data}
    \caption{Observed voltage trace of a current clamped neuron synthetically generated by the HH simulator simulated for $T = 120 \ms$ with time resolution $\Delta t=0.025 \ms$. The stimulus is a step current $I = 10 \, \mathrm{\mu A/cm}^2$ with onset and offset at $10 \ms$ and $110 \ms$, respectively. Here, the conductance parameters $\gbarK = 36 \gunit$ and $\gbarNa = 120 \gunit$. The present voltage trace being the observation, these conductance parameters are thus the ground truths for the subsequent analyses.}
    \label{fig:hh_obs_data}
\end{figure} 

From the voltage trace, we extract spike statistics by the computational algorithms outlined in \cref{sec:software}. \autoref{fig:hh_stat_extraction} shows the locations in the voltage trace that form the basis of the spike statistic calculations. In fact, the annotations on the voltage trace are set automatically according to the positions found by the extraction algorithms. By the definitions of the different summary statistics provided in \cref{sec:spike_statistics}, we see that the extraction locations are placed correctly on the voltage trace. Consequently, the extraction algorithms seem to function as intended. 

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.9]{hh_stat_extraction}
    \caption{Locations found by the feature extraction algorithms for spike statistic calculations on the observed voltage trace. The locations are annotated by different markers, with labels stated in the legend, which indicate the particular summary statistic calculation they are affiliated with.}
    \label{fig:hh_stat_extraction}
\end{figure} 

\autoref{tab:hh_obs_sumstats} summarizes the calculated summary statistics from the observed voltage trace; (i) \textit{spike rate}, calculated as the number of spikes divided by the duration of the stimulus; (ii) \textit{average AP overshoot}, calculated by averaging the absolute peak voltage of all APs; (iii) \textit{average AP width}, calculated by averaging the width of every AP at the midpoint between its onset and its peak; (iv) \textit{average AHP depth}, calculated by averaging all minima voltage throughs between two consecutive APs; (v) \textit{latency to first spike}, calculated as the time between stimulus onset and first AP peak; (vi) \textit{accommodation index}, which measures the local variance in ISIs and is calculated by \cref{eq:accomm_index}. Comparing the values of the tabulated summary statistics with the information in \autoref{fig:hh_stat_extraction}, we find agreement. There are 7 spikes over the course of the stimulus duration of $100 \ms$, so the spike rate must be $0.07 \, \mathrm{mHz}$. Furthermore, the value of the average AP overshoot and width, as well as the average AHP depth, seem reasonable when compared with the voltage values at the extracted locations. A latency to first spike of about $2 \ms$ also matches what is seen in the voltage trace. There is practically no difference in length between two consecutive ISIs in the voltage trace, and the accommodation index should therefore reflect, as it does, the lack of variability. All in all, this indicates that also the summary statistic calculations are implemented correctly. 

\begin{table}[!htb]
  \caption{Observed voltage trace reduced to a set of summary statistics. See text for details on the statistics.  }
  %\footnotesize%
  \begin{center}
    \rowcolors{2}{gray!15}{white}
    \begin{tabular}{cc}
      \toprule
      \textbf{Summary statistic} & \textbf{Observed value} \\
      \midrule
      %Number of spikes &  7 \\
      Spike rate &  0.0700 mHz \\
      Average AP overshoot & 30.7316 mV  \\
      Average AP width &  2.0501 mV \\
      Average AHP depth & -74.2234 mV \\
      Latency to first spike & 2.3000 ms \\
      Accommodation index &  $2 \cdot 10^{-17}$ \\
      \bottomrule
    \end{tabular}
  \end{center}
  \label{tab:hh_obs_sumstats}
\end{table}

%================================================================
\subsection{Correlation Analysis \& Importance Weights}
%================================================================

Next, we carry out the correlation analysis outlined in \cref{sec:corr_analysis}. The objective of the analysis is to characterize the effects of parameter variability on the output of the model in terms of the summary statistics. The analysis is done by sampling from the prior predictive distribution, and the priors for $\gbarK$ and $\gbarNa$ are shown in \autoref{fig:hh_priors}. 
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.8]{hh_priors}
    \caption{Priors over $\gbarK$ (top) and $\gbarNa$ (bottom). We use both an informative (blue) and noninformative (orange) centered about the ground truth parameter value (red line) for each parameter.}
    \label{fig:hh_priors}
\end{figure}
For each parameter, we use a noninformative prior (orange density) with about $\pm 10\%$ range around the ground truth parameter and a slightly more informative prior (blue density). While technically the informative priors could be classified as weakly-informative (as per the definition given in \cref{sec:coin_flipping}), we will refer to them as informative. 

For each category of priors, we sampled 2000 parameter pairs, fed them to the HH simulator model and calculated the summary statistics from the simulated data for each pair. The spike statistics are only well-defined in the presence of spikes, and accommodation index and average AHP depth need at least two and three spikes, respectively, to be defined. As such, we need to remove samples if they contain ill-defined statistics.

%================================================================
\subsubsection*{With Informative Priors}
%================================================================

Of the 2000 summary statistics samples simulated under the informative priors, 1881 were well-defined. Scatter plots for a subset of these are shown in \autoref{fig:hh_priorpred_sstats_normal}, where the summary statistics are shown as functions of the pairs of parameter values. Thus, the scatter plots enable us to see the variability the different summary statistics exhibit relative to change in model parameter values. Each point indicates the relative magnitude of the statistic by its size and color, with a reference table stated in the legend along with the name of the particular statistic.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{hh_priorpred_sstats_normal}
    \caption{Scatter plots of summary statistics simulated with different pairs of model parameter values. The summary statistics were simulated under the joint informative prior predictive distribution. Of 2000 generated samples, 1881 were well-defined. Here, a subset of 470 samples are shown. Each summary statistic is assigned to its own panel, with the particular statistic stated in the legend. Each point represents the value of a summary statistic for a pair of parameter values, $\qty(\gbarK, \gbarNa)$. The color of a point indicates the relative magnitude of the statistic, for which bright colors represent small and dark colors large values, also indicated in each subplot legend. The scatter plots thus indicate the variability of summary statistics relative to movement of the pairs of model parameter values.
    }
    \label{fig:hh_priorpred_sstats_normal}
\end{figure} 
In addition to displaying the variability, the scatter plots also indicate if there is a systematic relationship between a parameter and summary statistic. By fixing the value of one of the parameters and following its line-of-sight, we can assess whether the other parameter systematically increments or decrements a summary statistic. Average AP overshoot exhibits a steady variability and seems to follow an approximately linear trend. This also applies to the average AP width, though to a lesser extent. The approximate linear relationship is most apparent for $\gbarK$ in both cases. In terms of the underlying biophysical mechanisms, we would expect average AP overshoot to be most sensitive to $\gbarNa$ and average AP width to $\gbarK$. The role of the \Na channel in AP generation is well-established, and AP overshoot is tied to the fast \Na channel dynamics. Likewise, given the role of the \K channel in repolarizing the neuron after an AP, the average AP width is tied to the duration of the recovery period. While these expectations are not clearly present in the scatter plots, we should keep in mind that we only explore a fairly limited region of the parameter space. We should therefore be cautious in our interpretation of how well the scatter plots indicate sensitivity. Accommodation index has almost no variation, which might be unsurprising since it measures the local variance in ISIs. The constant current protocol does not facilitate much variation in spike trains. Thus, for the observed voltage trace at hand, accommodation index is not an informative summary statistic. However, it could be useful in characterizing spike trains generated under more complex current protocols. Average AHP depth also exhibits little variation, whereas latency to first spike an ample amount. Though, for latency to first spike there is no apparent systematic relationship, as it increments and decrements right and left. This might not be optimal for constraining the model parameters. Finally, the spike rate, though it does not vary much, seem to have a more systematic and stable relationship with the model parameters, predominantly $\gbarK$. It behaves more like a step function, where it retains a particular value for a prolonged range of $\gbarK$ values. In conclusion, how well this set of summary statistics will constrain the model parameters needs to be investigated further down the line.  

In order to not rely solely on visual inspection of the sensitivity, \autoref{fig:hh_weights_normal} provides the pairwise Pearson's correlation coefficients of each model parameter and summary statistics, as well as the importance weights derived from the correlation coefficients.
% subfigure
\begin{figure}[!htb]
\centering
\subfloat[]{{\includegraphics[scale=0.65]{hh_priorpred_corr_normal}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.65]{hh_priorpred_weights_normal}}}
\caption{\textbf{(a)} The pairwise Pearson's correlation coefficients for the model parameters and summary statistics. \textbf{(b)} The importance weights calculated from the correlation coefficients, see \cref{sec:corr_analysis} for details. Note that the weights sum to one.
}
\label{fig:hh_weights_normal}
\end{figure}
As was indicated by the scatter plots, $\gbarK$ has a stronger (approximately) linear relationship with the summary statistics than $\gbarNa$. The interpretation of these results are that the summary statistics related to the shape of an AP encode the most information, with average AP overshoot being the most dominant. Moreover, the results indicate that $\gbarK$ will be constrained better than $\gbarNa$ by these summaries when performing regression adjustment. Since the relationship between $\gbarK$ and the summary statistics is much stronger than for $\gbarNa$, the weighting scheme also becomes biased towards $\gbarK$. The weighting is therefore a bit unfair for $\gbarNa$. For instance, even though the spike rate is weakly correlated with $\gbarNa$, it receives a heavy weight because it is strongly correlated with $\gbarK$. The weight of the spike rate even surpasses that of average AP width, which shows a decent amount of correlation with both $\gbarNa$ and $\gbarK$. The pairwise Pearson's correlation coefficients strong assumption about linearity again necessitates the need to be wary of the interpretation of which summary statistics are the most informative. As the weighting scheme prefers $\gbarK$, we should also investigate the effects importance weights have on the inference further down the line.

%================================================================
\subsubsection*{With Noninformative Priors}
%================================================================

The findings with noninformative priors are similar to the ones discussed above. This is perhaps unsurprising, since a prior does not alter the intrinsic relationship between a parameter and summary statistic, just how the samples are distributed in the parameter space. The corresponding figures with samples from the joint noninformative prior distribution can be found in \cref{sec:Appendix A}.


%================================================================
\section{Study of ABC Settings}
%================================================================

We now turn to a study concerning the tuning parameters in the rejection ABC algorithm. As the generation of one posterior amounts to a single stochastic trial, we will generate several posteriors for the same settings in order to assess potential variability in the results. Here, we use the rejection sampler in \cw{pyLFI} to infer the conductance parameters in the HH model. The performance metrics RMSPE and SEM, defined in \cref{sec:performance_metrics}, will be used to assess the accuracy and variability, respectively, of a particular inference setting.

In our first tuning parameter analysis, we study the effect of the tolerance parameter $\epsilon$ in terms of the $p_\epsilon$-quantile of the distances. For each quantile, we generate 10 posterior with 1000 posterior samples in each. As discussed in \cref{sec:software}, this means that the tolerance will be set via a pilot study. For each quantile, the pilot study performs 2000 simulations to estimate both the tolerance and the scale of the summary statistics. In this analysis, the summary statistics are equally weighted. Having obtained a posterior, we perform local linear regression adjustment with the Epanechnikov kernel and log transformation of the parameters to obtain a corresponding adjusted posterior. We do the above using both the informative and noninformative priors. \autoref{fig:RMSPE_vs_quantile} shows the results.
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.8]{RMSPE_vs_quantile}
    \caption{The RMSPE in posteriors over $\gbarK$ (top) and $\gbarNa$ (bottom) against the $p_\epsilon$-quantile as a measure of tolerance. Each point is the mean RMSPE over 10 posteriors, each consisting of 1000 posterior samples, and the SEM is shown as a vertical bar. The posteriors were generated by the rejection ABC algorithm and then adjusted with local linear regression adjustment. Whether an estimate of error had informative/noninformative priors or is from the original/adjusted posterior is color coded (see legend).}
    \label{fig:RMSPE_vs_quantile}
\end{figure} 
RMSPE measures the percentage difference between the ground truth and a weighted estimate that accounts for the width of the posterior. Increasing $p_\epsilon$-quantiles amount to accepting simulated data that are increasingly further away from the observed data. As such, we expect the error to increase with $p_\epsilon$, due to more distorted approximations. This is also the general trend, though the differences in error between posteriors are generally small. Here, the posterior error estimates of $\gbarK$ and $\gbarNa$ differ. Focusing on the estimates of error in the original posterior samples, i.e., the samples obtained solely with the rejection ABC algorithm, $\gbarK$ has larger errors than $\gbarNa$. The errors of $\gbarNa$ remain almost constant as $p_\epsilon$ increases, and are actually slightly larger for the lowest $p_\epsilon$. The reason for this may be intricate, but most likely it has to do with correlation between the posterior samples of $\gbarNa$ and $\gbarK$ and that the simulated summary statistics accepted under a strict tolerance happen to shift the $\gbarNa$ estimate for the worse (by a little amount) and $\gbarK$ for the better. In terms of variation, the SEM of all the mean RMSPE shows that the inferred posteriors for the same settings are practically indistinguishable. We also see that estimates with informative priors converge better than those with noninformative priors, as expected. The RMSPE of the regression adjusted posterior estimates are significantly more accurate than the original posterior estimates. The improvement is most prominent for $\gbarK$, and aligns with the expectation obtained from the correlation analysis; since $\gbarK$ has a stronger linear relationship with the summary statistics than $\gbarNa$, the local linear regression model will give better adjustment of the $\gbarK$ posterior samples. The difference in error when $p_\epsilon$ increases are tiny for $\gbarK$, which suggests that the regression approach manages to correct the $\gbarK$ samples as if they were sampled from $\pi_\mathrm{ABC} \qty(\gbarK \mid \rho \qty(\ssim, \sobs) \leq \epsilon)$ with $\epsilon=0$. For $\gbarNa$, however, this breaks down and the error increases with $p_\epsilon$, with a particular jump between the 0.1 and 0.5-quantiles. Nevertheless, with regression adjustment, more simulations can be accepted without sacrificing substantial accuracy, especially for the model parameters that are the most constrained by the summary statistics. In addition, the difference in error between using informative and noninformative priors is also reduced when performing regression adjustment. 

Next, we investigate error in the estimates against the number of summary statistics used to constrain the model parameters. We start with only a single statistic, average AP overshoot, and increment by one more according to the following order; spike rate, average AP width, average AHP depth, latency to first spike and accommodation index. Again, we use the quantile-based rejection ABC algorithm with local linear regression adjustment using the Epanechnikov kernel to estimate the posteriors. We use the 0.4-quantile as a compromise between accuracy and run time (see \autoref{fig:runtime}). From the preceding result (\autoref{fig:RMSPE_vs_quantile}) we found that the posteriors generated for the same settings are practically identical. Here, we therefore only generate a single posterior for each number of summary statistics. This is, however, done for both the informative and noninformative priors. In addition, we generate posteriors for both cases of weighting of the summary statistics; either equally or importance weighted (we ensure the importance weights sum to one). The results are shown in \autoref{fig:RMSPE_vs_n_sumstats}.
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.8]{RMSPE_vs_n_sumstats}
    \caption{The RMSPE in posteriors over $\gbarK$ (top) and $\gbarNa$ (bottom) against the number of summary statistics. The first, single statistic is (i) average AP overshoot, and then the number of statistics is incremented by one more according to the following order; (ii) spike rate, (iii) average AP width, (iv) average AHP depth, (v) latency to first spike and (vi) accommodation index. Each point is the RMSPE in a regression adjusted posterior consisting of 1000 posterior samples. Whether an estimate of error had informative/noninformative priors or equally/importance weighted the summary statistics is color coded (see legend).
    }
    \label{fig:RMSPE_vs_n_sumstats}
\end{figure} 
The figure shows that the HH model is more tightly constrained by increasing the number of summary statistics, particularly $\gbarK$ as we have already discussed. Though, if we use summary statistics that do not capture relevant information for the parameters, it might lead to worse inference. The set of summary statistics that gives the most accurate posteriors consists of: (i) average AP overshoot, (ii) spike rate, (iii) average AP width and (iv) average AHP depth. For $\gbarK$, inclusion of the remaining two statistics, (v) latency to first spike and (vi) accommodation index, does not lead to a noticeable difference in accuracy. Neither does using equally or importance weighted summary statistics. For $\gbarNa$, on the other hand, the error becomes moderately worse by including (v) latency to first spike and (vi) accommodation index. Here, using importance weights actually helps to constrain $\gbarNa$ and improves the error in the posterior. 


%================================================================
\section{Summarizing Posteriors}
%================================================================

We can assess the identifiability of the HH model's active conductance parameters by examining the locations and widths of the resulting posterior estimates. A wide, flat posterior on a parameter indicates a large number of equally optimal values, which suggests that the parameter may be unidentifiable. As outlined in \cref{sec:performance_metrics}, the goodness of fit of the inferred posteriors will be considered through the MAP estimate, $95\%$ highest density interval (HDI) and, since we have access to the ground truths, RMSPE. We will also use posterior predictive checks (PPCs) to check for auto-consistency. To reiterate the settings of the rejection ABC sampler; we generate the following posteriors using the 0.4-quantile to determine the tolerance and the set of importance weighted summary statistics (i) average AP overshoot, (ii) spike rate, (iii) average AP width and (iv) average AHP depth.

Informed by the preceding findings, going forward we will use the set of summary statistics labelled (i)-(iv) above and also keep the inclusion of importance weights. 

%================================================================
\subsection{Posteriors from Informative Priors}
%================================================================

\autoref{fig:hh_posterior_org_normal} shows the original posteriors over $\gbarK$ and $\gbarNa$, with summarizing metrics stated in the legends. Compared with the informative priors over the model parameters (see \autoref{fig:hh_priors}), the updated state of knowledge represented by the posteriors is more constrained. The MAP estimates are centered close on the ground truth parameters, which, by the definition of MAP, means that the ground truth parameters are in regions of high posterior density. Compared to the $\gbarNa$ posterior, the RMSPE of the $\gbarK$ posterior is slightly larger, even though the 95\% HDI of $\gbarK$ is narrower than that of $\gbarNa$. This might be a bit surprising, but can be explained by noticing the sharpness of the posterior peaks. The peak of the $\gbarK$ posterior is more flat compared to the peak of the $\gbarNa$ posterior, which is quite sharp about the ground truth, and this is reflected in the RMSPE measure.
\begin{figure}[H]
    \centering
    \includegraphics[scale=1.0]{hh_posterior_org_normal}
    \caption{Original rejection ABC posteriors over the Hodgkin-Huxley model parameters $\gbarK$ (top) and $\gbarNa$ (bottom). Here, the parameter proposals were sampled from the joint informative prior distribution. The dark shaded region indicates the 95\% HDI and the dotted line the MAP estimate. The ground truth is indicated by the red line. The legend states the numerical values for each of these, in addition to the RMSPE in the posterior.}
    \label{fig:hh_posterior_org_normal}
\end{figure}

\autoref{fig:hh_joint} maps the correlation, in terms of pairwise Pearson's correlation coefficients, between the posterior samples of $\gbarK$ and $\gbarNa$. It shows that the parameter samples are indeed strongly correlated. Consequently, for predictive sampling from the posteriors, we need to sample from the \textit{joint posterior}, also shown in the figure.
% subfigure
\begin{figure}[!htb]
\centering
\subfloat[]{{\includegraphics[scale=0.5]{hh_corr_org_normal}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.65]{hh_joint_posterior_org_normal}}}
\caption{\textbf{(a)} The pairwise Pearson's correlation coefficients for the posterior samples of $\gbarK$ and $\gbarNa$. \textbf{(b)} The joint posterior distribution of $\gbarK$ and $\gbarNa$. Darker regions correspond to higher density, and the ground truth is indicated by the red marker and axis lines. Since the marginal posteriors over model parameters (shown on the marginal axes) are highly correlated, predictive posterior samples need to be sampled from the joint posterior. 
}
\label{fig:hh_joint}
\end{figure}

\autoref{fig:hh_posterior_reg_normal} shows the regression adjusted posterior. Here, $\gbarK$ is constrained to the interval $[35.986, 36.029]$ with 95\% probability, and $\gbarNa$ to the $[120.001, 120.423]$ with 95\% probability. The RMSPE in both posteriors are consequently reduced significantly compared with RMSPE in the original posteriors (\autoref{fig:hh_posterior_org_normal}). Again, we see that the corrected $\gbarK$ samples become more constrained than the $\gbarNa$ samples, due to the stronger linear relationship $\gbarK$ has with the summary statistics. The regression adjustment overshoots the ground truth of $\gbarNa$ by a tiny amount, but the estimated parameter range could be equally capable of describing the observed data. The regression adjusted posteriors are able to identify the conductance parameters remarkably well, which is promising for using ABC algorithms to identify parameters in other conductance-based neural models based on the HH formalism as well. 
\begin{figure}[H]
    \centering
    \includegraphics[scale=1.0]{hh_posterior_reg_normal}
    \caption{Regression adjusted rejection ABC posteriors over the Hodgkin-Huxley model parameters $\gbarK$ (top) and $\gbarNa$ (bottom). Here, the parameter proposals were sampled from the joint informative prior distribution. The dark shaded region indicates the 95\% HDI and the dotted line the MAP estimate. The ground truth is indicated by the red line. The legend states the numerical values for each of these, in addition to the RMSPE in the posterior.}
    \label{fig:hh_posterior_reg_normal}
\end{figure}

To verify that the parameter ranges in the regression adjusted posteriors are able to describe the observed data well, we perform a PPC. We draw 100 samples from the joint posterior predictive distribution and feed the parameters to the HH simulator. We then take the average of the simulated voltage traces. The result can be seen in \autoref{fig:hh_postpred_reg_normal}, where we plot the predicted simulations and their mean together with the observed voltage trace. 
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=1.0]{hh_postpred_reg_normal}
    \caption{Graphical posterior predictive check comparing the observed voltage trace to simulated data predicted by the Hodgkin-Huxley model under the regression adjusted joint posterior predictive distribution.}
    \label{fig:hh_postpred_reg_normal}
\end{figure}
As can be seen from the figure, the samples from the inferred joint posterior lead to simulations that are virtually identical to the observed data, confirming that the procedure succeeds at capturing the observed data and identifying the underlying parameters.


%================================================================
\subsection{Posteriors from Noninformative Priors}
%================================================================

By now, it has been demonstrated that regression adjustment of the posteriors is crucial for improving the accuracy of estimates from the rejection ABC sampler. \autoref{fig:hh_posterior_reg_uniform} shows the regression adjusted posteriors over $\gbarK$ and $\gbarNa$ when we use noninformative priors.
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=1.0]{hh_posterior_reg_uniform}
    \caption{Regression adjusted rejection ABC posteriors over the Hodgkin-Huxley model parameters $\gbarK$ (top) and $\gbarNa$ (bottom). Here, the parameter proposals were sampled from the joint noninformative prior distribution. The dark shaded region indicates the 95\% HDI and the dotted line the MAP estimate. The ground truth is indicated by the red line. The legend states the numerical values for each of these, in addition to the RMSPE in the posterior.}
    \label{fig:hh_posterior_reg_uniform}
\end{figure} 
Compared with the regression adjusted posteriors where we used informative priors, the present posteriors are only marginally less accurate. This means that, even with data-driven ABC inference, the HH conductance parameters can be accurately identified.


%================================================================
\section{SNPE Posteriors}
%================================================================

Rejection ABC is one of simplest simulation-based inference algorithms. We have added certain refinements to the standard rejection ABC sampling procedure, such as regression adjustment, weighted Euclidean distance and quantile-based rejection. Now, we will compare the posteriors obtained through our implementation of ABC in \cw{pyLFI} to one of the more recent advancements in the field; neural density estimation (NDE). In particular, the NDE algorithm SNPE introduced in \cref{sec:nde}. The objective here is not to perform an exhaustive analysis of SNPE and its capabilities, as this is demonstrated in the original papers \cite{SNL_first}, \cite{SNPE_first} and \cite{SNPE_apt}. Here, and in subsequent analyses with SNPE, the aim is to compare how its estimated posteriors compare with the ABC posterior under similar settings. This means that we will train the neural density estimator to learn the association between summary statistics of the data and the underlying parameters. As neural density estimator we use a particular normalizing flow (NF) called \textit{masked autoregressive flow} (MAF) that is developed by Papamakarios et al. \cite{MAF}. SNPE is given a modified HH simulator, which returns the same set of summary statistics we used in the preceding analyses; (i) average AP overshoot, (ii) spike rate, (iii) average AP width and (iv) average AHP depth. Moreover, we use the same noninformative priors as in \autoref{fig:hh_priors}, and train the network on 1000 simulations. The resulting posteriors over $\gbarK$ and $\gbarNa$ are shown in \autoref{fig:hh_post_sbi}.
\begin{figure}[H]
    \centering
    \includegraphics[scale=1.0]{hh_post_sbi}
    \caption{SNPE posteriors over the Hodgkin-Huxley model parameters $\gbarK$ (top) and $\gbarNa$ (bottom). The dark shaded region indicates the 95\% HDI and the dotted line the MAP estimate. The ground truth is indicated by the red line. The legend states the numerical values for each of these, in addition to the RMSPE in the posterior.}
    \label{fig:hh_post_sbi}
\end{figure}
Both posteriors we obtain are narrow and sharply peaked about the ground truth parameters, meaning that SNPE is able to identify admissible parameters as well. The summaries of the estimated posteriors are similar to the ones for the regression adjusted ABC posteriors. Though it is a close competition, the ABC posteriors are actually slightly more narrow than the SNPE posteriors. Moreover, the outliers observed in the tails of the SNPE posterior are not present in the ABC posteriors. This comparison might not be entirely fair to SNPE, as it is difficult to pin-point exactly how many training simulations that measure up to be “under similar settings”. By training the network on even more simulations, the SNPE posteriors could perhaps be made even more narrow. Furthermore, it is likely that the parameter ranges in the SNPE posteriors all are compatible with the observed data, which will be illuminated in more detail in the next section.


%PPC
%\begin{figure}[H]
%    \centering
%    \includegraphics[scale=0.8]{hh_post_pred_sbi}
%    \caption{caption}
%    \label{fig:fig1}
%\end{figure}

%see that the wider posterior has an effect on recreating the observed data, this is not present in rej-abc reg adjust posterior due to it being more narrow


%================================================================
\section{Noisy Observation}
%================================================================

So far we have only used an idealized voltage trace, free of any noise, as the observed data. However, real-world neural data are quite noisy. Here, we will examine the impact a noisy observed recording has on the inference with the rejection ABC sampler. There are several sources to noise in cellular dynamics, and, in extension, ways to introduce noise to the Hodgkin-Huxley equations, see e.g. \cite{hh_noise} for a review. We will introduce noise to the observed voltage trace by using a stochastic version of the HH model that incorporates current noise as a Gaussian white noise process. Besides the inclusion of current noise, we use the same settings for the HH simulator as earlier and record the voltage trace seen in \autoref{fig:hh_noisy_data}.
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.9]{hh_noisy_data}
    \caption{Noisy voltage trace generated with the HH simulator by introducing Gaussian white noise to the input stimulus. The parametrization of the HH model and simulation parameters are identical to the ones used in preceding noise-free voltage trace (\autoref{fig:hh_obs_data}).}
    \label{fig:hh_noisy_data}
\end{figure} 
The corresponding summary statistics are tabulated in \autoref{tab:hh_noisy_sumstats}. 
\begin{table}[!htb]
  \caption{Summary statistics extracted from the noisy observed voltage trace.}
  %\footnotesize%
  \begin{center}
    \rowcolors{2}{gray!15}{white}
    \begin{tabular}{cc}
      \toprule
      \textbf{Summary statistic} & \textbf{Observed value} \\
      \midrule
      Number of spikes &  7 \\
      Spike rate &  0.0700 mHz \\
      Average AP overshoot & 30.7223 mV  \\
      Average AP width & 2.0679 mV \\
      Average AHP depth & -74.3394 mV \\
      Latency to first spike & 2.2750 ms \\
      Accommodation index &  -0.0067 \\
      \bottomrule
    \end{tabular}
  \end{center}
  \label{tab:hh_noisy_sumstats}
\end{table}

We then use the rejection ABC sampler in \cw{pyLFI} and a HH simulator that generates noise-free simulations to infer the underlying parameters in the observed voltage trace. Though our observation is not a particularly noisy recording, it still distorts the regression adjusted ABC posteriors significantly compared to when we used noise-free observed data, as seen in \autoref{fig:hh_posterior_reg_noisy}.
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=1.0]{hh_posterior_reg_noisy}
    \caption{Regression adjusted rejection ABC posteriors over the Hodgkin-Huxley model parameters $\gbarK$ (top) and $\gbarNa$ (bottom) with noisy observed voltage trace. Here, the parameter proposals were sampled from the joint informative prior distribution. The dark shaded region indicates the 95\% HDI and the dotted line the MAP estimate. The ground truth is indicated by the red line. The legend states the numerical values for each of these, in addition to the RMSPE in the posterior.}
    \label{fig:hh_posterior_reg_noisy}
\end{figure} 
As can be seen, the ground truth parameters are no longer in regions of high posterior density -- they are not actually included in the posteriors at all. Though both the $\gbarK$ and $\gbarNa$ posteriors are narrow and sharp, their locations in parameter space are shifted toward a completely different set of parameter values, especially $\gbarNa$, than we found with the noise-free observation. However, if we do a graphical PPC, as seen in \autoref{fig:hh_postpred_reg_noisy}, we find that the simulations predicted by the joint posterior match the observed voltage trace surprisingly well. 
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=1.0]{hh_postpred_reg_noisy}
    \caption{Graphical posterior predictive check comparing the noisy observed voltage trace to simulated data predicted by the Hodgkin-Huxley model under the regression adjusted joint posterior predictive distribution. The posterior predictive mean is the average of 100 predicted simulations.}
    \label{fig:hh_postpred_reg_noisy}
\end{figure}
This example bolster the motivation for why the Bayesian approach to inference should be considered; there might be multiple parameter settings that are consistent with the observed data.




%===============================================================
%===============================================================
%===============================================================
%===============================================================
%===============================================================

%================================================================
\chapter{Inference on the Brunel Model}\label{chap:res_brunel}
%================================================================ 

In this chapter, we present the results from simulation-based inference on the Brunel network model's synaptic weight parameters $\eta$ and $g$. 

We will first try to identify the synaptic weight parameters with the quantile-based rejection ABC sampler in \cw{pyLFI} and post-sampling local linear regression adjustment with the Epanechnikov kernel. The observed data will be from the network's AI state, synthetically generated from the Brunel simulator in \cw{NeuroModels}. The parametrization of the Brunel network will be as given in \autoref{tab:bnet_model_parameters}.  

We will then try to identify the parameters with SNPE. Here, we will try to utilize the flexibility of SNPE by training on simulations from both the AI and SR states, and see what posteriors it will predict when presented with observed data from one of these states. 

%================================================================
\section{Inference with ABC}
%================================================================

%================================================================
\subsection{Observation from AI State}
%================================================================

We create a Brunel network with $10,000$ excitatory and $2,500$ inhibitory neurons. Each neuron is randomly connected with $1000$ excitatory and $250$ inhibitory neurons. The synaptic weight parameters are set as $\eta=2$ and $g=5$, which according to the phase diagram of the network (\autoref{fig:brunel_phase}) corresponds to the AI state. We simulate the network for $1,000 \ms$ and record the output spike trains from $20$ excitatory neurons. We start recording after $100 \ms$ to avoid transient effects. The observed network activity is shown in \autoref{fig:brunel_ai_observation}.
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=1.0]{brunel_ai_observation}
    \caption{Observed network activity recorded from $20$ excitatory neurons in the Brunel network's AI state. The synaptic weight parameters are set as $\eta=2$ and $g=5$, and the remaining parameters according to \autoref{tab:bnet_model_parameters}. The network is simulated for  $1,000 \ms$ and recording start after $100 \ms$. The top left panel shows the firing times (raster) of the recorded neurons, and the bottom left panel the network activity as a time resolved firing rate computed in bins of $10 \ms$. The mean firing rate is indicated by the horizontal (red) axis line. The right panel shows the pairwise Pearson's correlation coefficient matrix of the recorded neurons, which is a measure of how synchronous the spiking of the network is. 
    }
    \label{fig:brunel_ai_observation}
\end{figure}
As seen in the figure, neurons in the AI state are weakly correlated and fire irregularly at low rates. The observed activity is reduced to the set of low-dimensional summary statistics outlined in \cref{sec:spiketrain_statistics}; (i) mean firing rate; (ii) mean CV; (iii) Fano factor. \autoref{tab:brunel_ai_sumstats} tabulates the calculated summary statistics from the observed activity.
\begin{table}[!htb]
  \caption{Observed summary statistics calculated from a network in the AI state.}
  %\footnotesize%
  \begin{center}
    \rowcolors{2}{gray!15}{white}
    \begin{tabular}{cc}
      \toprule
      \textbf{Summary statistic} & \textbf{Observed value} \\
      \midrule
      Mean firing rate &  0.0366 kHz \\
      Mean CV &  0.4250  \\
      Fano factor & 0.2341  \\
      \bottomrule
    \end{tabular}
  \end{center}
  \label{tab:brunel_ai_sumstats}
\end{table}
Each statistic captures different aspects of the activity. The mean firing rate is a direct measure of the population's spiking activity, mean CV measures the regularity of spike trains and Fano factor the variability across spike trains. 

%correlation (pearson) coefficient matrix

%\begin{figure}[H]
%    \centering
%    \includegraphics[scale=1.0]{brunel_obs_corr}
%    \caption{caption}
%    \label{fig:fig1}
%\end{figure}

%================================================================
\subsection{Correlation Analysis \& Importance Weights}
%================================================================

As we did for the inference on the HH model, we first assess how sensitive the summary statistics are to movement in parameter values, and then measure the relationship between them with a correlation analysis. This analysis is done by sampling from the prior predictive distribution, and the priors for $\eta$ and $g$ are shown in \autoref{fig:brunel_priors}. 
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.8]{brunel_priors}
    \caption{Priors over $\eta$ (top) and $g$ (bottom). We use noninformative priors with ranges corresponding to the AI state. The ground truth parameters are indicated by the red lines. 
    }
    \label{fig:brunel_priors}
\end{figure}
We have here chosen noninformative priors, with parameter ranges that correspond to the AI state, as seen in the phase diagram over the Brunel network's states (\autoref{fig:brunel_phase}). We then draw 2000 samples from the joint prior predictive distribution, feed each parameter pair to the Brunel simulator and calculate the resulting summary statistics from each simulation. \autoref{fig:brunel_sum_stats} shows a subset of the generated samples as scatter plots, where each point is the simulated statistic for a given pair of parameter values. 
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.75]{brunel_sum_stats}
    \caption{Scatter plots of summary statistics simulated with different pairs of model parameter values. The summary statistics were simulated under the joint prior predictive distribution. Here a subset of 500 samples out of 2000 is shown. Each summary statistic is assigned to its own panel, with the particular statistic stated in the legend. Each point represents the value of a summary statistic for a pair of parameter values, $\qty(g, \eta)$. The color of a point indicates the relative magnitude of the statistic, for which bright colors represent small and dark colors large values, also indicated in each subplot legend. The scatter plots thus indicate the variability of summary statistics relative to movement of the pairs of model parameter values.
    }
    \label{fig:brunel_sum_stats}
\end{figure}
The relative magnitude of a statistic is indicated by its size and color, with a reference table stated in the legend along with the name of the particular statistic. The scatter plots show that all the summary statistics exhibit a steady variability. In particular, for the model parameter $g$, which controls the amount of inhibition in the network, the relationship with each of the statistics seems to follow an approximately linear trend. However, for $\eta$, which determines the strength of the external drive, a distinct systematic relationship with the statistics is less pronounced. From a biophysical point of view, this is not entirely surprising. AI activity is a hallmark of recurrent networks, and arise from that excitation is balanced by inhibition. It can persist even in the absence of external input. As such, the dynamics of the network tend to be more dependent on $g$ than $\eta$, and we can expect, at least with this set of summary statistics, that $g$ will be constrained better than $\eta$.

The pairwise Pearson's correlation coefficients in the left panel of \autoref{fig:brunel_sum_stats_weights} confirm the observation of a stronger linear relationship between the summary statistics and $g$ than with $\eta$.
\begin{figure}[!htb]
\centering
\subfloat[]{{\includegraphics[scale=0.5]{brunel_sum_stats_corr}}}
\qquad
\subfloat[]{{\includegraphics[scale=0.5]{brunel_sum_stats_weights}}}
\caption{\textbf{(a)} The pairwise Pearson's correlation coefficients for the model parameters and summary statistics. \textbf{(b)} The importance weights calculated from the correlation coefficients, see \cref{sec:corr_analysis} for details. Note that the weights sum to one.
}
\label{fig:brunel_sum_stats_weights}
\end{figure}
Furthermore, as seen in the right panel, the importance weights for the summary statistics calculated from the correlation coefficients have roughly the same magnitude. This implies that all the summary statistics encode useful information about the activity for constraining the model parameters, and that the information they encode is nearly equally important. 


%================================================================
\subsection{ABC Settings}
%================================================================

The Brunel simulator has costly simulations. We will therefore use the approach where we set a \textit{simulation budget}, discussed in \cref{sec:software}. This means that the simulator only will run the specified amount of times and the number of posterior samples we retain depend on which quantile is set. This is in contrast to the approach we used for the HH model, where a pilot study was used to estimate the tolerance based on the provided quantile and the ABC sampler used this tolerance for obtaining the specified amount of posterior samples. To examine the impact the choice of quantile has on the accuracy of the posteriors, we use a simulation budget of $2,000$ model simulations and compute the RMSPE for different choices of quantile. The result is shown in \autoref{fig:brunel_quantile_rmspe}. As expected, increasing the $p_\epsilon$-quantile gives an increase in the RMSPE, since simulations further away from the observed data are accepted. Moreover, we see once more that regression adjustment of the posteriors is necessary to obtain accurate estimates. We also see that $g$ is more constrained by the summary statistics than $\eta$, which falls in line with our expectations from the previous analysis. However, regression adjustment significantly helps to constrain $\eta$ as well.  

In the subsequent analyses, we keep the simulation budget of $2,000$ simulations. As a compromise between accuracy and the number of samples in the posterior, we will use the $0.3$-quantile as a measure of tolerance.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{brunel_quantile_rmspe}
    \caption{The RMSPE in posteriors over $\eta$ (top) and $g$ (bottom) against the $p_\epsilon$-quantile as a measure of tolerance. A simulation budget of $2,000$ was used, and the number of posterior samples that are retained for a particular $p_\epsilon$-quantile is indicated by the marker inside the point. The corresponding label in the legend states the number of posterior samples retained for that particular quantile. The posteriors were generated by the rejection ABC algorithm and then adjusted with local linear regression adjustment. Whether an estimate of error is from the original/adjusted posterior is color coded (see legend).
    }
    \label{fig:brunel_quantile_rmspe}
\end{figure}


%================================================================
\subsection{Summarizing Posteriors}
%================================================================


The regression adjusted posteriors over $\eta$ and $g$ are shown in \autoref{fig:brunel_posterior_reg}. 
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=1.0]{brunel_posterior_reg}
    \caption{Regression adjusted rejection ABC posteriors over the Brunel network model parameters $\eta$ (top) and $g$ (bottom) with observed data from the AI state. The dark shaded region indicates the 95\% HDI and the dotted line the MAP estimate. The ground truth is indicated by the red line. The legend states the numerical values for each of these, in addition to the RMSPE in the posterior.
    }
    \label{fig:brunel_posterior_reg}
\end{figure}
In the updated state of knowledge, the prior range of $\eta \in \qty[1.5, 4.0]$ has been constrained to $\sim \eta \in \qty[1.6, 2.1]$ with $95\%$ probability, whereas the prior range of $g \in \qty[4.0, 8.0]$ has been constrained to $\sim g \in \qty[4.7, 5.3]$ with $95\%$ probability. Even though both ground truth parameters lie in regions of high posterior density, it is only $g$ that is identified moderately accurate. The posterior over $g$ is sharply peaked, though it is not particularly narrow. The posterior over $\eta$, on the other hand, has a relatively flat peak compared to $g$, and the identification of $\eta$ is therefore less successful as a wide and flat posterior indicates a large number of equally optimal values. Although this aligns with our biophysical expectations, these results illustrate the limitation of the rejection ABC approach where proposal parameters are only sampled from the prior. By using a sampler which updates the proposals recursively, like the MCMC ABC algorithm discussed extensively in this thesis (see e.g. \cref{sec:abc}), more efficient sampling of parameters from high density regions can be achieved. 

\autoref{fig:brunel_joint_posterior_reg} illustrates the joint posterior over $\eta$ and $g$. It can be seen that the joint ground truth (red marker) lies in a region of high posterior density. 
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.9]{brunel_joint_posterior_reg}
    \caption{The regression adjusted joint posterior distribution over $\eta$ and $g$ in the AI state. Darker regions correspond to higher density, and the ground truth is indicated by the red marker and axis lines. Since the marginal posteriors over model parameters (shown on the marginal axes) are highly correlated, predictive posterior samples need to be sampled from the joint posterior. 
    }
    \label{fig:brunel_joint_posterior_reg}
\end{figure}


%================================================================
\subsection{Posterior Predictive Checks}
%================================================================

To see whether particular summaries of the network activity are mapped accurately by the posterior predictions, we perform a graphical PPC. We draw 50 samples from the regression adjusted joint posterior predictive distribution and feed the parameters to the Brunel simulator. We then calculate the summary statistics, i.e., (i) mean firing rate; (ii) mean CV; (iii) Fano factor, for each of the output spike trains. The simulated summary statistics and their average together with the corresponding observed summary statistic are then plotted in summary statistic space. The result can be seen in \autoref{fig:brunel_post_pred}.
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.8]{brunel_post_pred}
    \caption{Graphical posterior predictive check comparing the observed summary statistics with simulated summary statistics predicted by the Brunel model under the regression adjusted joint posterior predictive distribution. The posterior predictive mean is the average of 50 predicted simulations.
    }
    \label{fig:brunel_post_pred}
\end{figure}
From the figure it can be seen that most of the predicted summary statistics are in the neighborhood of the observation, but they can also be relatively far away. This is not unexpected though. Due to the stochastic nature of spike generation in networks, activity simulated under the exact same settings will differ among themselves to a significant degree. This intrinsic variability may have an important functional role in biological neural networks, but pose a challenge for fitting models to data. Since the target observations themselves are variable, the approach used here, where we fit to just a single observation, might not be the best method. Even though we are able to constrain the model parameters to some extent, the intrinsic variability of the observation, and hence the underlying data generating process, should be taken into account before any general insights can be proclaimed. 




%The pairwise Pearson's correlation coefficient is a measure of how synchronous the spiking of a network is. This correlation coefficient measures the correlation between the spike trains of two neurons in the network. In Figure X we examine how this correlation depends on parameter uncertainties by plotting the mean and standard deviation for the pairwise Pearson's correlation coefficient in the AI state. 

%\autoref{fig:brunel_pred_corr}
% subfigure
%\begin{figure}[!htb]
%\centering
%\subfloat[]{{\includegraphics[scale=0.6]{brunel_pred_corr}}}
%\qquad
%\subfloat[]{{\includegraphics[scale=0.6]{brunel_pred_corr_std}}}
%\caption{\textbf{(a)} mean correlation coefficient matrix. \textbf{(b)} standard deviation
%}
%\label{fig:brunel_pred_corr}
%\end{figure}



%================================================================
\section{Inference with SNPE}
%================================================================

%================================================================
\subsection{Training the Neural Density Estimator}
%================================================================

how we trained 

use the same configuration as for HH

%================================================================
\subsection{Inference in the AI State}
%================================================================

Same observed AI data

\autoref{fig:brunel_post_ai_sbi} Posterior:

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=1.0]{brunel_post_ai_sbi}
    \caption{caption}
    \label{fig:brunel_post_ai_sbi}
\end{figure}

\autoref{fig:brunel_post_pred_ai_sbi} ppc 

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.8]{brunel_post_pred_ai_sbi}
    \caption{caption}
    \label{fig:brunel_post_pred_ai_sbi}
\end{figure}

%corr + corr std \autoref{fig:brunel_pred_corr_sbi_ai}
% subfigure
%\begin{figure}[!htb]
%\centering
%\subfloat[]{{\includegraphics[scale=0.6]{brunel_pred_corr_sbi_ai}}}
%\qquad
%\subfloat[]{{\includegraphics[scale=0.6]{brunel_pred_corr_std_ai_sbi}}}
%\caption{\textbf{(a)} mean correlation coefficient matrix. \textbf{(b)} standard deviation
%}
%\label{fig:brunel_pred_corr_sbi_ai}
%\end{figure}

%================================================================
\subsection{Inference in the SR State}
%================================================================

We create another Brunel network with the same number of neurons and connections as earlier, but this time we set the synaptic weight parameters as $\eta=2$ and $g=3$, which according to the phase diagram of the network (\autoref{fig:brunel_phase}) corresponds to the SR state. We again simulate the network for $1,000 \ms$ and start to record the output spike trains from $20$ excitatory neurons after $100 \ms$. The observed network activity is shown in \autoref{fig:brunel_sr_observation}.
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=1.0]{brunel_sr_observation}
    \caption{Observed network activity recorded from $20$ excitatory neurons in the Brunel network's SR state. The synaptic weight parameters are set as $\eta=2$ and $g=3$, and the remaining parameters according to \autoref{tab:bnet_model_parameters}. The network is simulated for  $1,000 \ms$ and recording start after $100 \ms$. The top left panel shows the firing times (raster) of the recorded neurons, and the bottom left panel the network activity as a time resolved firing rate computed in bins of $10 \ms$. The mean firing rate is indicated by the horizontal (red) axis line. The right panel shows the pairwise Pearson's correlation coefficient matrix of the recorded neurons, which is a measure of how synchronous the spiking of the network is. 
    }
    \label{fig:brunel_sr_observation}
\end{figure}

high spiking activity due to the lowered inhibition allows for stronger recurrent input, that becomes a positive feedback loop

---
In the AI state, neurons fire mostly independently at low rates. It is characterized by that neurons in the population fire at different times (\textit{asynchronous firing}) and at irregular intervals. 

Depending on the strength of this external drive and the synaptic coupling parameters g and J, spiking activity can be asynchronous and irregular

the sustained asynchronous activation of groups of neurons in the absence of external drive together with the highly irregular spiking of individual ce

Further, it is unclear whether waves are consistent with the low rate and weakly correlated “asynchronous-irregular” dynamics observed in cortical recordings. 
---

Recorded spike trains from the Brunel network and observed the following summary statistics: \autoref{tab:brunel_sr_sumstats}

\begin{table}[!htb]
  \caption{SR state observation.}
  %\footnotesize%
  \begin{center}
    \rowcolors{2}{gray!15}{white}
    \begin{tabular}{cc}
      \toprule
      \textbf{Summary statistic} & \textbf{Observed value} \\
      \midrule
      Mean firing rate &  0.3333 kHz \\
      Mean CV &  0.0121  \\
      Fano factor & 0.007  \\
      \bottomrule
    \end{tabular}
  \end{center}
  \label{tab:brunel_sr_sumstats}
\end{table}

prior: However, we will try to utilize the flexibility of SNPE by training on simulations from both the AI and synchronous regular (SR) state, to investigate whether the predicted posteriors, when using observed data from one of these states, match the expected parameter ranges from the phase diagram (\autoref{fig:brunel_phase}).

\autoref{fig:brunel_post_sr_sbi} Posterior:

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.8]{brunel_post_sr_sbi}
    \caption{caption}
    \label{fig:brunel_post_sr_sbi}
\end{figure}

width of posterior not that important, cool thing is that the range that corresponds to the state is constrained (check if true)

\autoref{fig:brunel_post_pred_sr_sbi} ppc 

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{brunel_post_pred_sr_sbi}
    \caption{caption}
    \label{fig:brunel_post_pred_sr_sbi}
\end{figure}


%corr + corr std \autoref{fig:brunel_pred_corr_sbi_sr}

% subfigure
%\begin{figure}[!htb]
%\centering
%\subfloat[]{{\includegraphics[scale=0.6]{brunel_pred_corr_sbi_sr}}}
%\qquad
%\subfloat[]{{\includegraphics[scale=0.6]{brunel_pred_corr_std_sr_sbi}}}
%\caption{\textbf{(a)} mean correlation coefficient matrix. \textbf{(b)} standard deviation
%}
%\label{fig:brunel_pred_corr_sbi_sr}
%\end{figure}






