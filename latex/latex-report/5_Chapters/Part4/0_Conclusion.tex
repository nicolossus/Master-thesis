%================================================================
\chapter{Summary}\label{chap:summary}
%================================================================

Mechanistic models of neural dynamics are poorly suited for inference and lead to challenging inverse problems. The objective of this thesis is to investigate the ability and utility of simulation-based inference for identifying parameters in mechanistic models of neural dynamics. We have used two simulation-based inference algorithms; (i) \textit{Approximate Bayesian Computation} (ABC) with quantile-based rejection sampling and post-sampling regression adjustment; (ii) the neural density estimator \textit{Sequential Neural Posterior Estimation} (SNPE); to infer model parameters in the Hodgkin-Huxley model \cite{HH1952} and the Brunel network model \cite{Brunel2000}. The primary focus of the study was on ABC, and SNPE results are mostly for comparison. In this chapter, we summarize our findings. 

%================================================================
\section{Correlation Analysis}
%================================================================ 

What we did, and what we found, why pearson might not be the best approach

As with covariance itself, the measure can only reflect a linear correlation of variables, and ignores many other types of relationship or correlation. 

---

correlation analysis results are indicating related results to the sensitivity analysis. Correlation analysis can be used in linear cases for the determination of important inputs on certain outputs. Here stands the condition that a meaningful number of observations are taken into account for the calculation of the empirical covariance and variances.

---

Sensitivity analysis

In a model with uncertain variables, you might want to know how much each uncertain input contributes to the uncertainty in the output. Typically, a few uncertain inputs are responsible for the lion’s share of the uncertainty in the output, while the rest have little impact. You can then concentrate on getting better estimates or building a more detailed model for the one or two most important inputs without spending considerable time investigating issues that turn out not to matter very much.

---

---

pearson correlation coefficient

Use Pearson's Coefficient of Correlation to weight importance of summary statistics.

* This is a simple approach for producing weighted statistics 
    * Method developed by H. Jung and P. Marjoram (2011) is more advanced and likely better
* The correlation coefficient, $r$, relates $Y$ to $X$ 
* The squared correlation coefficient, $r^2$, indicates the proportion of variance in $Y$ that is shared with (or accounted for) by $X$.

**Cons of using Pearson's Coefficient of Correlation:**

* Assumes:
    1. Normality of data (meaning that the data should approximate the normal distribution; most data points should tend to hover close to the mean)
    2. Homoscedasticity (means ‘equal variances’), i.e. a situation in which the variance of the dependent variable is the same for all the data.
    3. Linearity; simply means that the data follows a linear relationship. 
* (2. and 3. can be checked visually by scatter plot)
* Is sensitive to outliers; outliers can can significantly skew the correlation coefficient and make it inaccurate. Outliers are also easy to spot visually from the scatter plot

* vekte utifra sensitivitet

%================================================================
\section{About MCMC ABC in this Study}
%================================================================ 

because MCMC methods have been instrumental in advancing Bayesian statistics

MCMC-ABC was also implemented in pyLFI, however, in order to focus the study on the objective, it is not included in the following analyses as it would not give any significant insights into the questions we seek than the more naive rejection sampler would. 

MCMC methods require a great deal of tuning and diagnosis in order to ensure that the chains actually have converged towards the stationary distribution and sample efficiently (has good mixing).

The use of MCMC ABC would add an unnecessary layer of complexity to the study.

Since the focus of this study is not the tuning of the ABC samplers to that extent, we will use the basic rejection sampler for the following analyses. MCMC ABC will not give any other insights that rejection ABC cannot answer for the study at hand 

However, we decided to include it in such detail because of the revolution MCMC methods brought to Bayesian statistics and that many of the advanced ABC samplers actually build on the MCMC approach. 

Although neither rejection nor mcmc abc are not the best methods for sampling accurately and efficiently, and scale poorly to higher dimensional problems, they provide a solid basis for understanding more sophisticated/refined sampling schemes and post-hoc adjustments. They showcase that abc provides a robust framework for parameter identification

%================================================================
\section{Approximate Bayesian Computation}
%================================================================ 

One of the principal topics of research in likelihood-free inference is how to obtain state-of-the-art results with fewer simulations. By dissecting and studying the methods in great detail, another aim is to contribute to this research within the time and scope a master thesis permits. 

ABC: Sampling and rejection methods. In this essay, two types of ABC methods were discussed: the vanilla rejection ABC, and the more sophisticated variant Markov chain Monte Carlo ABC. There are, however, more advanced refinements that can be explored further, such as partial least squares dimension reductions for summaries and post-processing regression adjustments. An excellent point of departure for material on improvements is the video lecture by Nott [11].
• ABC: General pitfalls and remedies. ABC has been successfully applied to a wide range of problems with an complex or absent associated likelihood. There are, however, several pitfalls to be aware of. The approach is simulation intensive, requires tuning of the tolerance threshold, discrepancy function and weighting function, and suffers from a curse of dimensionality of the summary statistic [5, p. 322]. Therefore, refinements to the methods that may remedy some of the pitfalls should be explored. A point of departure could be [5] and its suggested further reading. 

---

A major methodological advance in ABC is that rather than implementing the algorithm described above, parameter sets can be more heavily weighted if the resulting simulation more closely matches the summaries of real data (Beaumont et al. 2002). In this framework, one can accept parameter values that are not as close a match to the empirical data, increasing the efficiency of the ABC. This framework has been implemented in local linear regression models (Beaumont et al. 2002) as well as other nonlinear regression approaches (Blum \& Francois 2010). 

ABC methods have several disadvantages. Firstly, the methods are highly computationally intensive, often involving billions of coalescent simulations, which may limit the parameter space explored and can lead to incorrect inference. However, the simplest ABC approaches are easily parallelizable and computational power is continuing to increase, potentially ameliorating this concern. Secondly, ABC approaches may not be efficient if the prior distributions are wide or if the type of model being considered is unlikely to generate the observed data. Under these conditions, a high proportion of parameter values will be rejected, although the local regression  approach discussed above will mitigate some of this concern. A more serious drawback is that the success of ABC inference may depend on the choice of summary statistics. If the summary statistics are not capturing enough information from the data or the relevant information for the parameters of interest, then the posterior distributions will appear similar to the prior distribution, suggesting that the data are noninformative. 


The tuning necessary is also a challenge

---

ABC methods were borne out of a need to tackle problems that defy conventional statistical methodologies. It has become clear, however, that whenever suitable Bayesian alternatives that do deal with the proper likelihood are available, ABC becomes computationally too expensive. The reason for this is primarily the fact that the representation of the posterior (as a weighted sum over Dirac delta functions) is not very efficient. So when alternatives are available, they ought to be used. In parallel to their role in computationally demanding applications, ABC techniques have, more recently, also attracted attention as an inferential framework in their own right [16,51]. From this, interesting new approaches to deal with real-world problems may well emerge [52].

In conclusion, ABC-based methods are best suited to those problems for which other likelihood-based (or exact) Bayesian inference procedures do not yet exist. This appears to still include a host of challenging and interesting problems. Many stochastic and highly structured spatio-temporal problems in ecology, epidemiology, and evolutionary genetics clearly fall into this category. The recent developments discussed above mean that ABC has become a viable new way of tackling computationally demanding parameter inference problems. Given a model—as long as we can simulate it—ABC gives us a handle to evaluate approximate posterior distributions, which then can be further evaluated. Sensitivity and robustness analyses, but also predictions of future behavior or the likely effects of any interventions or perturbations, can be analyzed by simulating the model with parameters sampled from the posterior. There is enormous scope for basing the exploration of, for example, policy or conservation measures on the available data in this way. ABC has, for example, been used in experimental design [50,53] and in synthetic biology [14,54] to generate designs of molecular pathways that exhibit certain types of behavior. In such cases, we replace the observed data, D, by a representation of the desired behavior (such as the desired abundance of a species). Then the inference procedure is used to identify the scenario for which we are most likely to observe this outcome. Such predictions then reflect the best available evidence in light of the data and the model.


%================================================================
\subsection{Regression adjustment}
%================================================================

A crucial limitation of the rejection-sampling method is that only a small number of summary statistics can usually be handled. Otherwise, either acceptance rates become prohibitively low or the tolerance $\epsilon$ must be increased, which can distort the approximation., because $\ssim$ are treated equally whenever $\rho \qty(\ssim, \sobs) \leq \epsilon$ irrespective of the precise value of $\rho \qty(\ssim, \sobs)$. Here, we introduce two improvements to existing rejection sampling methods, smooth weighting and regression adjustment. The key benefit is insensitivity of the approximation to $\epsilon$. This insensitivity permits increasing the number of summary statistics, thus potentially increasing the information extracted from the data. 

With summary statistic $T_1$, the adjustment is able to correct for the bias caused by the nonzero tolerance $\epsilon$. With summary statistic $T_2$, the tolerance is too large for accurate adjustment, although the result is still closer to the target distribution than the original.

individual components of $\theta$ are treated separately, thus not accounting for possible correlations between them. Fig. X shows that the posterior samples indeed are correlated.

Also nonlinear models have been proposed, see e.g. Blum (2010)

%================================================================
\subsection{Choice of ABC Algorithm}
%================================================================

MCMC-ABC was also implemented in pyLFI, however, in order to focus the study on the objective, it is not included in the following analyses as it would not give any significant insights into the questions we seek than the more naive rejection sampler would. 

Although neither rejection nor mcmc abc are not the best methods for sampling accurately and efficiently, and scale poorly to higher dimensional problems, they provide a solid basis for understanding more sophisticated/refined sampling schemes and post-hoc adjustments. They showcase that abc provides a robust framework for parameter identification

other choices for the method include the prior distributions and settings of the ABC algorithm. We used uninformative priors to demonstrate the accuracy of the method based on data alone. However, including informative priors would speed up the convergence of the algorithm. 


The efficiency of the method is, however, largely dependent on the efficieny of the simulator model. As an informal side note: all results in this thesis were produced on a personal laptop. Inference on the most time-consuming model, the Brunel network, took about 2 hours with ABC (2000 simulations) and 2.5 hours with SNPE (running 1000 (adaptively proposed) simulations to train the network). 

rejection abc not efficient if the prior .. posterior. MCMC, as discussed in theory section, improves on this by ... 

There are more advanced importance sampling ABC methods that builds on the MCMC approach: SMC, PMC, HMC - ABC (link to papers). must monitor convergence

Perhaps the most notable difference between the approach of ABC and SNPE, is that SNPE uses \textit{all} model simulations to train the network opposed to ABC that usually ends up rejecting most of the simulations. Another important difference is that once the network is trained, it can be used to predict posterior on any empirical data with just one pass through the network. Whereas ABC would need to be performed again when presented with another set of empirical data. 


other choices for the method include the prior distributions and settings of the ABC algorithm. We used uninformative priors to demonstrate the accuracy of the method based on data alone. However, including informative priors would speed up the convergence of the algorithm. 

The method can be used for a broad class of models where the likelihood is not known or difficult to compute, This is a great advantage in the model development as models can potentially be calibrated before their derivation is finalized.

The proposed method is computationally lightweight and can be run on standard laptops with reasonable run-time. The computation time is dominated by the particular model evaluation time. Thus the computational cost depends heavily on the specific model and its implementation

%================================================================
\subsection{Choice of Summary Statistics}
%================================================================

The choice of summary statistics is vital in determining the outcome of the inverse modelling with ABC. The set of statistics must effectively encode all phenomena of the original data that the experimenter wishes to be modelled. 

For our purposes, requiring that S.Y/ be sufficient is problematic because we
can’t examine an unknown or intractable likelihood to determine if the Fisher- Neyman Factorization Theorem holds. Although there are a growing number of different strategies for resolving the sufficiency problem [42], the most common approach has been to select a large set of summary statistics and hope that this set of statistics is large enough that it is at least close to being jointly sufficient for the parameters of interest. While adding more summary statistics to this set will

tend to provide more information about theta, sufficiency can still never be guaranteed, and some summary statistics may provide identical information about the model parameters. When a set of summary statistics are not sufficient for the parameters, then the influence of the information conveyed by the observed data will be weaker, resulting in posterior distributions that are inaccurate, particularly with respect to the degree of variability in the estimated posteriors


---

A further motivation for this approach is that real-world experiments often are interested in capturing summary statistics of the experimental data. 

In practice, in order to maintain the acceptance probability at manageable levels, we typically transform the data into a small number of features, also known as summary statistics, in order to reduce the dimensionality D. If the summary statistics are sufficient for inferring $\theta$, then turning data into summary statistics incurs no loss of information about $\theta$ (for a formal definition of sufficiency, see e.g. Wasserman, 2010, definition 9.32). However, it can be hard to find sufficient summary statistics, and so in practice summary statistics are often chosen based on domain knowledge about the inference task. A large section of the ABC literature is devoted to constructing good summary statistics (see e.g. Blum et al., 2013; Charnock et al., 2018; Fearnhead and Prangle, 2012; Prangle et al., 2014, and references therein).


figure x shows how the Hodgkin-Huxley model is more tightly constrained by increasing the number of summary statistics. Though, if we use summary statistics that do not capture relevant information for the parameters of interest it might lead to worse inference, as seen in the figure for the latency to first spike and accommodation index statistics.  

However, this result showcases the importance of carrying out a sensitivity analysis on the model parameters. Uncertainpy

explanatory power

the success of ABC inference depends on the choice of summary statistics. If the summary statistics are not capturing enough information from the data or the relevant information for the parameters of interest, then the posterior distributions will appear similar to the prior distribution, suggesting that the data are noninformative. 

--- 
To deal with high dimensional data, ABC algorithms typically reduce them to lower dimensional summary statistics and accept when simulated summaries are close to the observed summaries. 

A crucial limitation of the ABC approach is that only a small number of summary statistics can usually be handled. Otherwise, either acceptance rates or become prohibitively low or the tolerance must be increased, which can distort the approximation. The problem is related to the general issue of the curse of dimensionality: many statistical tasks are substantially more difficult in high dimensional settings. 


--- 

Summary statistics may be used to increase the acceptance rate of ABC algorithms, and low-dimensional sufficient statistics are optimal for this purpose, as they capture all relevant information present in the data in the simplest possible form. However, low-dimensional sufficient statistics are typically unattainable for models where ABC-based inference is most relevant, and consequently, some heuristic is usually necessary to identify useful low-dimensional summary statistics. [We performed a correlation analysis, blabla, then checked the error as a function of number of sum stats->not good approach as the accuracy and stability of ABC decreases rapidly with an increasing number of summary statistics]. Ref to sensitivity analysis and MOO. (The use of poorly chosen summary statistics will often lead to inflated credible intervals due to the implied loss of information).

---

When fitting mechanistic models to data, it is common to target summary statistics to isolate specific behaviors, rather than the full data. For example, the spike shape is known to constrain potassium and sodium conductances. When modeling population dynamics, it is often desirable to achieve realistic firing rates or rate-correlations. Choice of summary statistics might also be guided by known limitations of either the model or the measurement approach, or necessitated by the fact that published data are only available in summarized form. 

Many features of interest in neural models, e.g., the latency to first spike after stimulus onset, are only well defined in the presence of other features, e.g. the presence of spikes. 

%================================================================
\subsection{Choice of Distance Metric}
%================================================================ 

While the choice of summary statistics is of primary importance, the choice of distance metric can also have a substantial impact on the quality of the posterior approximation. In the present study, we used the Euclidean distance. This amounts to a circular acceptance region, as illustrated by \autoref{fig:abc_illustration}, which implies independence and identical scales of the summary statistics. Technically, because of the dimensionality of the summary statistics vectors we used, the acceptance regions were spherical. However, we stick with the circular analogy as it is easier to conceptualize. In order to suppress domination of the summary statistics with largest scale, we used a weighted version of the Euclidean distance that scaled the summary statistics with their respective standard deviation estimated from the prior predictive distribution. Moreover, we also weighted the importance of the summary statistics based on the correlation analysis. Despite these efforts, the Euclidean distance might not have the acceptance region that results in the most accurate ABC posterior approximation. If we, reasonably, suppose that the different summary statistics are dependent and on different scales, their true distribution under the model may be better represented by an elliptical acceptance region rather than a circular. The Mahalanobis distance is an example of a distance metric with elliptical acceptance region. However, the different levels of information encoded by the different summary statistics make finding the optimal acceptance region a challenge. Even with different scales and dependences, a circular acceptance region might result in a more accurate ABC posterior approximation than an elliptical region if the circular is better tied up with the most informative statistics. How the best acceptance region is connected with the choice of summary statistics is discussed in detail by Prangle in \cite{prangle_distance}. 



%================================================================
\section{Identifiability}
%================================================================  

To assess the goodness of the inferred solution, we have generated an image with known con- figuration and compared the estimated posterior with the input configuration. We find that ABC produced a reliable approximate posterior that was consistent with the input param- eter values and mapped the correlation between simulation parameters. The ABC method with its PMC implementation is thus promising for numerous forward modeling problems in cosmology and astrophysics.

To assess the goodness of the inferred posterior, we have discussed several considerations for good performance, including the MAP estimate, highest density interval and posterior predictive checks. Particularly, since the ground truth parameters are known, we have used an error measure that considers the width of the posterior by averaging over the posterior, RMSPE.

see HH ABC 

We will assess the identifiability of the potassium and sodium conductance parameters by examining the width of the resulting posterior estimates. A wide, flat posterior on a parameter indicates a large number of equally optimal values, which suggests that the parameter may be unidentifiable.

Application of these new techniques also allows us to explore the issue of model identifiability. By examining posterior distributions over model parameters, we can quantify and characterize model uncertainty, which can either inform us as to the ability of our data to constrain the model or the degree of biological variability in the system, as we outline below.

If a cellular model were to have a non-unique optimal parametrization, with either several or infinitely many equally likely possibilities, the model may be deemed 'unidentifiable'. Unidentifiability can be divided into two types: structural unidentifiability, where the model is overly complex to describe the system (this is often called over-parametrization), and practical unidentifiability, where there are not enough data to fully constrain the model. Distinguishing between the two types of unidentifiability can be important for experimental design. Pinpointing a structural  unidentifiability, which is characterized by a functional relationship between several parameters (and thus infinitely many equally optimal parametrizations), may be cause for concern, and prompt changes in model formulation before attempting further experimentation. However, if there is a high degree of prior faith in the model formulation, this may instead suggest a biologically important redundancy in the system characterized by the functional relationship between biophysical parameters. Practical unidentifiability, on the other hand, may inform how additional experimental data might be collected in an effort to further constrain the model, or may be a representation of inherent variability of the biophysical parameters in the experimental system. 

%================================================================
\subsection{The Hodgkin-Huxley Model}
%================================================================ 

Hodgkin-Huxley, reason for why $\bar{g}_\mathrm{K}$ is better constrained than $\bar{g}_\mathrm{Na}$:

The sensitivity analysis reveals that the variance in the membrane potential mainly is due to the uncertainty in two parameters: the maximum conductance of the $\mathrm{K}^+$ channel, $\bar{g}_\mathrm{K}$, and the $\mathrm{Na}^+$ reversal potential, $E_\mathrm{Na}$.

A similar study of using ABC to constrain the Hodgkin-Huxley model parameters was carried out in .. hh abc .. They primarily focused on constraining the rate parameters, and were also able to successfully identify the parameters.  

Indicating that Abc is a viable method for parameter identification in the many models using the hodgkin-huxley formalism

%================================================================
\subsection{The Brunel Network Model}
%================================================================ 

Brunel 

Able to accurately predict summary statistics ...


Indicates that $\eta$ is not dominant in determining the dynamics of the SR state. There is a biophysical explanation to this observation. In the SR state, the recurrent input from the network is significantly larger than that of the external input. With this positive-feedback network, it is unsurprising that we are not able to constrain $\eta$ as it is a parameter that describes the external input, ... overdominance of recurrent input from the network. 

Supported by the findings of (uncertainpy paper), where a sensitivity analysis of the Brunel model parameters was carried out. In the SR state, (uncertainpy paper) found that the Brunel network is most sensitive to the synaptic delay $D$ (which is kept constant in our results). However, the network was also found to be more sensitive to $g$ than $\eta$ in the SR state. 

In the AI state, the network was found to be most sensitive to the relative strength of inhibitory synapses $g$. The $\eta$ parameter also plays a role in the AI state, but not as much as $g$. The synaptic delay $D$ plays no role. 

SNPE both Brunel states

Expected unidentifiability: SR state Brunel, biophysical explanation. In this state of neurons firing almost synchronously at high rates, the spiking activity is to a large extent generated by the network population itself due to the recurrent connections. (Positive feedback). Thus, will the external input play a lesser role. The $\eta$ parameter that describes the amount/strength of external input (background rate) will therefore be overshadowed by the internal drive of the activity if the internal rate is significantly larger. Also seen in/confirmed by Uncertainpy analysis.

%================================================================
\subsection{Constraining Model Parameters}
%================================================================ 


When fitting mechanistic models to data, it is common to target summary features to isolate specific behaviors, rather than the full data. For example, the spike shape is known to constrain sodium and potassium conductances (Druckmann et al., 2007; Pospischil et al., 2008; Hay et al., 2011). When modeling population dynamics, it is often desirable to achieve realistic firing rates, rate-correlations and response nonlinearities (Rubin et al., 2015; Bittner et al., 2019), or specified oscillations (Prinz et al., 2004). Choice of summary features might also be guided by known limitations of either the model or the measurement approach, or necessitated by the fact that published data are only available in summarized form.  In all cases, care is needed when interpreting models fit to summary features, as choice of features can influence the results (Blum et al., 2013; Jiang et al., 2017; Izbicki et al., 2019).


While there is no universally accepted automated means to assess identifiability, many different methods, such as parameter sensitivity analysis and analysis of curvature of an objective function, have been applied to cardiac cell models.   


- parameter sensitivity analysis: uncertainpy 

- analysis of curvature of an objective function: druckmann

... have been applied to models of neural dynamics

\textbf{Relevant papers:}

H. Jung and P. Marjoram (2011). "Choice of Summary Statistic Weights in Approximate Bayesian Computation". \url{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3192002/}

* Develops a Genetic Algorithm that computes how one should weight the summary statistics 

* Fairly advanced, so we won't implement their approach, but should be mentioned in thesis

%================================================================
\section{Scalability}
%================================================================ 

Addressing limitations of ABC

It is known that ABC will not perform well in all cases and faces issues common across all model optimization problems. All optimization algorithms are subject to the so-called curse of dimensionality, by which the exponential increase in the volume of the search space with each new parameter effectively mean that any sampling procedure will in practice be extremely sparse. 

High-dimensional datasets and high-dimensional parameter spaces can require an extremely large number of parameter points to be simulated in ABC-based studies to obtain a reasonable level of accuracy for the posterior inferences. In such situations, the computational cost is severely increased and may in the worst case render the computational analysis/investigation intractable. These are examples of well-known phenomena which are usually referred to with the umbrella-term curse-of-dimensionality \cite{ABCprimer}. 

machine learning based may 

---

We now turn to a discussion of scalability ... not investigated in this thesis. 

SNPE has also been applied by the creators on several models of neural dynamics in \cite{SNPE_applied}. They were able to use SNPE to identify up to ... parameters. (scalability)


We have only demonstrated the success of simulation-based inference on low dimensional problems, i.e., few parameters to identify. 

state-of-the-art V1 allen model, 1500 free parameters. Although it might be excessive to infer all of these parameters at once, the problem will nonetheless be high-dimensional. 

ABC have been shown to scale poorly to higher dimensions

SNPE have been demonstrated to obtain good results for systems with about X free parameters (ref paper), but has yet to demonstrate scalability to problems like the V1 model. In its current state, one of the principal investigators behind SNPE, J. Macke, stated that SNPE is only able to infer up to a couple of dozen parameters, depending on the problem (ref: in conference).


\section{Notes}

In this thesis, we have formulated a framework for the inverse modelling of neural dynamics based upon an algorithm using rejection ABC with post-hoc corrections through regression adjustment. We characterized the properties of this method when applied to models typically used in computational neuroscience; a biophysically detailed neuron model (HH model) and a neural network of point neurons model (Brunel model). Examination of the outcomes of the rejection ABC algorithm indicated that parameter estimates converge to yield best fit approximations to the summary statistics of empirical data and provide constraints upon the parameter estimates associated with a data dependent reduction in the estimated posterior precision. 

To assess the validity of the procedure we tested for predictive validity. These results demonstrated that the model inversion and comparison approach are able to reliably identify the model that generated the data. 

%================================================================
\chapter{Conclusions}\label{chap:conclusions}
%================================================================

We investigated how simulation-based inference can be applied to inverse modelling of mechanistic models of neural dynamics, where the likelihood is unavailable or intractable. In particular, we explored the ability of approximate Bayesian computation (ABC) and neural density estimation (NDE) to identify the conductance parameters in the Hodgkin-Huxley model and the synaptic weight parameters in the Brunel network model. 

We discussed an implementation of the rejection ABC algorithm with quantile-based tolerance and post-sampling local linear regression adjustment. Even though rejection ABC just sample proposal parameters from the prior densities, we were, in general, successful in identifying the model parameters of interest. By applying local linear regression adjustment, we obtained narrow posterior densities with the ground truth values of the parameters in regions of high posterior density. The posterior error decreases in the limit of small tolerances. With regression adjustment, we found that we could accept more simulations without sacrificing substantial accuracy for the model parameters that were the most constrained by the summary statistics. The choice of summary statistics is crucial for the performance of ABC. In practice, the success of ABC relies on expert-crafted low-dimensional summary statistics that constrain the model parameters of interest. Although the rejection ABC approach were able to perform efficiently, in particular on the Hodgkin-Huxley model, and recover the ground truth parameters, more advanced sampling algorithms with better proposal mechanisms should be considered. The accurate results we were able to obtain with the simple rejection sampler implies that even better results can be expected by using more advanced samplers. 

We compared the rejection ABC algorithm with the NDE machine learning tool Sequential Neural Posterior Estimation (SNPE), which trains an artificial neural network to map features of observed data to posteriors over parameters by using adaptively proposed model simulations. Inference on the Brunel network model demonstrates the power and flexibility of SNPE; by training on simulations that included the parameter ranges for two of the network states, SNPE was able to accurately predict posteriors that corresponded to the network's state in the observed data. The advantage of the SNPE approach is that once trained, the network can be applied to any observed data and estimate the posterior densities over model parameters with only a single pass through the network. 


%Recently, there have been developed simulation-based inference machine learning algorithms using \textit{artificial neural network-based conditional density estimators}. In particular, the  \textit{Sequential Neural Posterior Estimation} (SNPE) algorithm targets parametrically learning the posteriors over model parameters by using adaptively proposed model simulations instead of likelihood calculations. More specifically, the algorithm trains a Bayesian mixture-density network (MDN) to learn the association between data, or summary statistics of the data, and underlying parameters. Instead of filtering out simulations, as ABC algorithms do, SNPE uses \textit{all} simulations to train the MDN to identify admissible parameters. Once trained, the network can then be applied to observed data to derive the posterior densities over the parameters of the simulator model. 


A limiting factor of the simulation-based approach to inference is that the algorithms are simulation intensive. Consequently, the efficiency of an algorithm predominantly depends on the computational demands of the simulator. There are also challenges (and opportunities) ahead in scaling and automating simulation-based inference approaches. However, the frontier of simulation-based inference as a methodological research field is rapidly advancing. This activity is promising for the numerous inverse modelling problems in neuroscience. The generality of the simulation-based inference methods allows them to be applied to a wide range of computational investigations in need of improved inference quality. In addition to being able to aid in designing better models of neural dynamics, simulation-based inference may ultimately help to bridge the gap between mechanistic hypotheses and experimental neural data. 


%However, simulation-based inference is promising for the numerous inverse modelling problems in neuroscience. The generality of the methods allows them to be applied to a wide range of computational investigations in need of improved inference quality. In addition to being able to aid in designing better models of neural dynamics, simulation-based inference may help to bridge the gap between mechanistic hypotheses and experimental neural data. 

%A limiting factor of the ABC methods are that they are simulation intensive and the efficiency of the procedure depends on the the computational demands of the simulator. ABC has been successfully applied to a wide range of problems with an complex or absent associated likelihood. There are, however, several pitfalls to be aware of. The approach is simulation intensive, requires tuning of the tolerance threshold, discrepancy function and weighting function, and suffers from a curse of dimensionality of the summary statistic. 

% Until recently, scientists confronted with inverse problems and a complex simulator as a forward model had little recourse other than to choose ABC or methods based on classical density estimation techniques. While these approaches have served some domains of science quite well, they have relied heavily on the labor-intensive process of experts providing powerful summary statistics. As a result, there is a frontier beyond which these traditional methods are less useful and new techniques are needed.

% The term likelihood-free inference has served as a point of convergence for what were previously disparate communities, and a new lingua franca has emerged. This has catalyzed significant cross-pollination and led to a renaissance in simulation-based inference. The advent of powerful ML methods is enabling practitioners to work directly with high-dimensional data and to reduce the reliance on expert-crafted summary statistics. New programming paradigms such as probabilistic programming and differentiable programming provide new capabilities that enable entirely new approaches to simulation-based inference. Finally, taking a more systems-level view of simulation-based inference that brings together the statistical and computational considerations has taken root. Here active learning is leading the way, but we expect more advances like this as simulation-based inference matures.

%The rapidly advancing frontier means that several domains of science should expect either a significant improvement in inference quality or the transition from heuristic approaches to those grounded in statistical terms tied to the underlying mechanistic model. It is not unreasonable to expect that this transition may have a profound impact on science.



%================================================================
\chapter{Future Research}\label{chap:future}
%================================================================

%Armed with these insights .. conclusion

We here provide an outline of potential future research building off of the present work.  

In this study, we have only investigated low-dimensional problems, i.e., inferential tasks with few parameters to identify. However, 

Recent studies, see e.g. Skaar et al. 

%Concluding remarks.

Possible future studies. Reference other relevant studies. 

Use more refined ABC methods
(SMC ABC ++, SNPE and SNL)

Compare with principled modelling; thermodynamic models

Metamodelling

Scalability 

Check if is scalable in the number of parameters;
Higher dimensional problems (more parameters to estimate)

Try to make contact between real-world experimental data and mechanistic models of neural dynamics. 
