%================================================================
\chapter{Summary}\label{chap:summary}
%================================================================

%================================================================
\chapter{Conclusions}\label{chap:conclusions}
%================================================================

Armed with these insights

We now turn to a discussion of

%================================================================
\chapter{Future Research}\label{chap:future}
%================================================================


%================================================================
\chapter{Summary \& Conclusions}\label{chap:Conclusion}
%\chapter{Conclusion \& Future Research}\label{chap:Conclusion}
%================================================================

Introduce chapter briefly, remind reader of objective and what has been done. 

In this thesis, we aim assess the ability and utility of simulation-based inference on mechanistic models of neural dynamics. We do this by using approximate Bayesian computation (ABC) with quantile-based rejection sampling and post-sampling local linear regression adjustment and snpe ... to investigate the identifiability of the Hodgkin-Huxley model and Brunel network model. 

We have used the simulation-based inference algorithms called approximate Bayesian computation and snpe.

---

SNL thesis

In practice, in order to maintain the acceptance probability at manageable levels, we typically transform the data into a small number of features, also known as summary statistics, in order to reduce the dimensionality D. If the summary statistics are sufficient for inferring $\theta$, then turning data into summary statistics incurs no loss of information about $\theta$ (for a formal definition of sufficiency, see e.g. Wasserman, 2010, definition 9.32). However, it can be hard to find sufficient summary statistics, and so in practice summary statistics are often chosen based on domain knowledge about the inference task. A large section of the ABC literature is devoted to constructing good summary statistics (see e.g. Blum et al., 2013; Charnock et al., 2018; Fearnhead and Prangle, 2012; Prangle et al., 2014, and references therein).

Sum stat discussion (from SNPE paper)

When fitting mechanistic models to data, it is common to target summary features to isolate specific behaviors, rather than the full data. For example, the spike shape is known to constrain sodium and potassium conductances (Druckmann et al., 2007; Pospischil et al., 2008; Hay et al., 2011). When modeling population dynamics, it is often desirable to achieve realistic firing rates, rate-correlations and response nonlinearities (Rubin et al., 2015; Bittner et al., 2019), or specified oscillations (Prinz et al., 2004). Choice of summary features might also be guided by known limitations of either the model or the measurement approach, or necessitated by the fact that published data are only available in summarized form.  In all cases, care is needed when interpreting models fit to summary features, as choice of features can influence the results (Blum et al., 2013; Jiang et al., 2017; Izbicki et al., 2019).


\section{Identifiability} 

see HH ABC 

We will assess the identifiability of the potassium and sodium conductance parameters by examining the width of the resulting posterior estimates. A wide, flat posterior on a parameter indicates a large number of equally optimal values, which suggests that the parameter may be unidentifiable.

Application of these new techniques also allows us to explore the issue of model identifiability. By examining posterior distributions over model parameters, we can quantify and characterize model uncertainty, which can either inform us as to the ability of our data to constrain the model or the degree of biological variability in the system, as we outline below.

If a cellular model were to have a non-unique optimal parametrization, with either several or infinitely many equally likely possibilities, the model may be deemed 'unidentifiable'. Unidentifiability can be divided into two types: structural unidentifiability, where the model is overly complex to describe the system (this is often called over-parametrization), and practical unidentifiability, where there are not enough data to fully constrain the model. Distinguishing between the two types of unidentifiability can be important for experimental design. Pinpointing a structural  unidentifiability, which is characterized by a functional relationship between several parameters (and thus infinitely many equally optimal parametrizations), may be cause for concern, and prompt changes in model formulation before attempting further experimentation. However, if there is a high degree of prior faith in the model formulation, this may instead suggest a biologically important redundancy in the system characterized by the functional relationship between biophysical parameters. Practical unidentifiability, on the other hand, may inform how additional experimental data might be collected in an effort to further constrain the model, or may be a representation of inherent variability of the biophysical parameters in the experimental system. 

While there is no universally accepted automated means to assess identifiability, many different methods, such as parameter sensitivity analysis and analysis of curvature of an objective function, have been applied to cardiac cell models.   


- parameter sensitivity analysis: uncertainpy 

- analysis of curvature of an objective function: druckmann

... have been applied to models of neural dynamics

---


\section{ABC}

ABC discussion 

One of the principal topics of research in likelihood-free inference is how to obtain state-of-the-art results with fewer simulations. By dissecting and studying the methods in great detail, another aim is to contribute to this research within the time and scope a master thesis permits. 


When in doubt, multiple prior distributions can be considered to assess the robustness of the results. 

The resulting summary statistics (multiple statistics are allowed) from the simulated data set are compared to those from the empirical data, and if close enough, the parameter values used to simulate the data are retained and contribute to the posterior distribution. If, however, the simulated summary statistics are not close in value to those of the empirical data, the parameters are rejected. This process is repeated until enough replicates (at least a few thousand) are retained. 

A major methodological advance in ABC is that rather than implementing the algorithm described above, parameter sets can be more heavily weighted if the resulting simulation more closely matches the summaries of real data (Beaumont et al. 2002). In this framework, one can accept parameter values that are not as close a match to the empirical data, increasing the efficiency of the ABC. This framework has been implemented in local linear regression models (Beaumont et al. 2002) as well as other nonlinear regression approaches (Blum \& Francois 2010). 

ABC methods have several disadvantages. Firstly, the methods are highly computationally intensive, often involving billions of coalescent simulations, which may limit the parameter space explored and can lead to incorrect inference. However, the simplest ABC approaches are easily parallelizable and computational power is continuing to increase, potentially ameliorating this concern. Secondly, ABC approaches may not be efficient if the prior distributions are wide or if the type of model being considered is unlikely to generate the observed data. Under these conditions, a high proportion of parameter values will be rejected, although the local regression  approach discussed above will mitigate some of this concern. A more serious drawback is that the success of ABC inference may depend on the choice of summary statistics. If the summary statistics are not capturing enough information from the data or the relevant information for the parameters of interest, then the posterior distributions will appear similar to the prior distribution, suggesting that the data are noninformative. 


The tuning necessary is also a challenge

The efficiency of the method is, however, largely dependent on the efficieny of the simulator model. As an informal side note: all results in this thesis were produced on a personal laptop. Inference on the most time-consuming model, the Brunel network, took about 2 hours with ABC (2000 simulations) and 2.5 hours with SNPE (running 1000 (adaptively proposed) simulations to train the network). 

Having found the optimal settings for the Hodgkin-Huxley model, the final posteriors only needed the results of all the all simulations presented

\section{Sum stats}

figure x shows how the Hodgkin-Huxley model is more tightly constrained by increasing the number of summary statistics. Though, if we use summary statistics that do not capture relevant information for the parameters of interest it might lead to worse inference, as seen in the figure for the latency to first spike and accommodation index statistics.  

However, this result showcases the importance of carrying out a sensitivity analysis on the model parameters. Uncertainpy

explanatory power

the success of ABC inference depends on the choice of summary statistics. If the summary statistics are not capturing enough information from the data or the relevant information for the parameters of interest, then the posterior distributions will appear similar to the prior distribution, suggesting that the data are noninformative. 

Expected unidentifiability: SR state Brunel, biophysical explanation. In this state of neurons firing almost synchronously at high rates, the spiking activity is to a large extent generated by the network population itself due to the recurrent connections. (Positive feedback). Thus, will the external input play a lesser role. The $\eta$ parameter that describes the amount/strength of external input (background rate) will therefore be overshadowed by the internal drive of the activity if the internal rate is significantly larger. Also seen in/confirmed by Uncertainpy analysis.

\section{Weighting scheme}

pearson correlation coefficient

Use Pearson's Coefficient of Correlation to weight importance of summary statistics.

* This is a simple approach for producing weighted statistics 
    * Method developed by H. Jung and P. Marjoram (2011) is more advanced and likely better
* The correlation coefficient, $r$, relates $Y$ to $X$ 
* The squared correlation coefficient, $r^2$, indicates the proportion of variance in $Y$ that is shared with (or accounted for) by $X$.

**Cons of using Pearson's Coefficient of Correlation:**

* Assumes:
    1. Normality of data (meaning that the data should approximate the normal distribution; most data points should tend to hover close to the mean)
    2. Homoscedasticity (means ‘equal variances’), i.e. a situation in which the variance of the dependent variable is the same for all the data.
    3. Linearity; simply means that the data follows a linear relationship. 
* (2. and 3. can be checked visually by scatter plot)
* Is sensitive to outliers; outliers can can significantly skew the correlation coefficient and make it inaccurate. Outliers are also easy to spot visually from the scatter plot

* vekte utifra sensitivitet

\textbf{Relevant papers:}

H. Jung and P. Marjoram (2011). "Choice of Summary Statistic Weights in Approximate Bayesian Computation". \url{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3192002/}

* Develops a Genetic Algorithm that computes how one should weight the summary statistics 

* Fairly advanced, so we won't implement their approach, but should be mentioned in thesis

\section{ABC methods}

rejection abc not efficient if the prior .. posterior. MCMC, as discussed in theory section, improves on this by ... 

There are more advanced importance sampling ABC methods that builds on the MCMC approach: SMC, PMC, HMC - ABC (link to papers)


\section{Uncertainpy}

Hodgkin-Huxley, reason for why $\bar{g}_\mathrm{K}$ is better constrained than $\bar{g}_\mathrm{Na}$:

The sensitivity analysis reveals that the variance in the membrane potential mainly is due to the uncertainty in two parameters: the maximum conductance of the $\mathrm{K}^+$ channel, $\bar{g}_\mathrm{K}$, and the $\mathrm{Na}^+$ reversal potential, $E_\mathrm{Na}$.

Brunel 

Able to accurately predict summary statistics ...

Indicates that $\eta$ is not dominant in determining the dynamics of the SR state. 

Supported by the findings of (uncertainpy paper), where a sensitivity analysis of the Brunel model parameters was carried out. In the SR state, (uncertainpy paper) found that the Brunel network is most sensitive to the synaptic delay $D$ (which is kept constant in our results). However, the network was also found to be more sensitive to $g$ than $\eta$ in the SR state. 

In the AI state, the network was found to be most sensitive to the relative strength of inhibitory synapses $g$. The $\eta$ parameter also plays a role in the AI state, but not as much as $g$. The synaptic delay $D$ plays no role. 

\section{Scalability}

We have only demonstrated the success of simulation-based inference on low dimensional problems, i.e., few parameters to identify. 

state-of-the-art V1 allen model, 1500 free parameters. Although it might be excessive to infer all of these parameters at once, the problem will nonetheless be high-dimensional. 

ABC have been shown to scale poorly to higher dimensions

SNPE have been demonstrated to obtain good results for systems with about X free parameters (ref paper), but has yet to demonstrate scalability to problems like the V1 model. In its current state, one of the principal investigators behind SNPE, J. Macke, stated that SNPE is only able to infer up to a couple of dozen parameters, depending on the problem (ref: in conference).


%----------------------------------------------------------------
\section{Summary}\label{sec:summary}
%----------------------------------------------------------------

Summarize main findings. Connect results to the literature. 

%----------------------------------------------------------------
\section{Conclusion}\label{sec:conclusion}
%----------------------------------------------------------------

Concluding remarks.

A limiting factor of the ABC methods are that they are simulation intensive and the efficiency of the procedure depends on the the computational demands of the simulator.  
ABC has been successfully applied to a wide range of problems with an complex or absent associated likelihood. There are, however, several pitfalls to be aware of. The approach is simulation intensive, requires tuning of the tolerance threshold, discrepancy function and weighting function, and suffers from a curse of dimensionality of the summary statistic.  

%----------------------------------------------------------------
\section{Future Research}\label{sec:future}
%----------------------------------------------------------------

Possible future studies. Reference other relevant studies. 

Use more refined ABC methods
(SMC ABC ++, SNPE and SNL)

Compare with principled modelling; thermodynamic models

Metamodelling

Scalability 

Check if is scalable in the number of parameters;
Higher dimensional problems (more parameters to estimate)

