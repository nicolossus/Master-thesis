%================================================================
\section{Structure of the Thesis}
%\section{How This Thesis is Organized}
%================================================================

%How This Book is Organized (ISL)

%Our view is that one must understand simple methods before trying to grasp more complex ones. Hence, after giving an overview of the supervising learning problem in Chapter 2, we discuss linear methods for regression and classification in Chapters 3 and 4. In Chapter 5 we describe splines, wavelets and regularization/penalization methods for a single predictor, while Chapter 6 covers kernel methods and local regression. Both of these sets of methods are important building blocks for high-dimensional learning techniques. Model assessment and selection is the topic of Chapter 7, covering the concepts of bias and variance, overfitting and methods such as cross-validation for choosing models. Chapter 8 discusses model inference and averaging, including an overview of maximum likelihood, Bayesian inference and the bootstrap, the EM algorithm, Gibbs sampling and bagging, A related procedure called boosting is the focus of Chapter 10.
%In Chapters 9â€“13 we describe a series of structured methods for supervised learning, with Chapters 9 and 11 covering regression and Chapters 12 and 13 focusing on classification. Chapter 14 describes methods for unsupervised learning. Two recently proposed techniques, random forests and ensemble learning, are discussed in Chapters 15 and 16. We describe undirected graphical models in Chapter 17 and finally we study high- dimensional problems in Chapter 18.
%At the end of each chapter we discuss computational considerations important for data mining applications, including how the computations scale with the number of observations and predictors. Each chapter ends with Bibliographic Notes giving background references for the material.

The thesis is organized into four parts. 

In \autoref{part:theory} we provide the theoretical background. First, we introduce Bayesian inference in general in \cref{chap:bayesian} and then simulation-based inference along with the algorithms we will use in \cref{chap:sbi}. Next, we give a brief introduction to neurobiology in \cref{chap:neurobio} before presenting the neuroscientific models that will be used in the study in \cref{chap:compneuro}. 

In \autoref{part:method} we discuss the methodologies we will use in the study. The specifics of the methodologies are given in \cref{chap:methodology} and the specifics of the computational approach in \cref{chap:comp_approach}. 

In \autoref{part:results} we present and discuss the results. \cref{chap:res_hh} presents the results on the Hodgkin-Huxley model and \cref{chap:res_brunel} the results on the Brunel network model. 

In \autoref{part:summary} we summarize our findings and conclusions. In \cref{chap:summary} we put our findings in perspective before concluding in \cref{chap:conclusions}. In \cref{chap:future} we provide an outline of potential future research. 

