%================================================================
\chapter{Introduction}
%\chapter{Introduction and Objective of the Study}
%================================================================ 

(Set context)

The human brain, with its 100 billion neurons, .. complicated 

This project is placed in the junction between biology, physics, numerical modeling and statistical learning 


At one of the junctions between biology, physics and numerical modeling we computational neuroscience. 

Computational neuroscience ... junction between biology, physics and numerical modeling. 

Computational neuroscience and machine learning. 

The following introduces the fundamental problem and objective of the master project and the methodology that will be applied as well.

%================================================================
\section{Motivation}\label{sec:Motivation}
%================================================================

Mechanistic models in neuroscience aim to explain neural or behavioral phenomena in terms of causal mechanisms, and candidate models are validated by investigating whether proposed mechanisms can explain how experimental data manifests. The mechanistic modelling is generally through the use of differential equations, and these models often have non-measurable parameters. A central challenge in building a mechanistic model is to identify the parametrization of the system which achieves an agreement between the model and experimental data. 

Finding well-fitted parameters by inspection becomes more difficult as the complexity of both data and models increase, and automated identification of data-compatible parameters become necessary. 

Statistical inference provides the mathematical means and procedures for automated parameter identification.  Statistical inference uses the \textit{likelihood function} to quantify the match between parameters and data by deriving estimators of the parameters from the data. In statistical inference, there are, broadly speaking, two paradigms for the analysis of sampled data: \textit{frequentist} inference and \textit{Bayesian} inference. In Bayesian inference, posterior beliefs about parameters are updated according to \textit{Bayes' theorem} upon observing data. Bayesian inference differs from the traditional frequentist inference by the fundamental interpretation of probability. In terms of parameter inference, the frequentist view is to regard the value of some parameter as fixed but unknown, whereas the Bayesian approach to inference is to regard the parameter as a random variable having a prior probability distribution. Consequently, one of the most important features of Bayesian inference is that it allows for uncertainty quantification of predictions. One could argue that it is essential in data analysis to not only provide a good model but also an uncertainty estimate of the conclusions. 

Many mechanistic models are defined implicitly through \textit{simulators}, i.e. a set of dynamical equations and possibly a description of sources of stochasticity, which can be run forward to generate data. Likelihoods can be derived for purely statistical models, but are generally intractable or computationally infeasible for simulation-based models. Hence are traditional methods in the toolkit of statistical inference inaccessible for many mechanistic models.

To overcome intractable likelihoods, a suite of methods that bypass the evaluation of the likelihood function, called \textit{likelihood-free inference} methods, have been developed. These methods seek to directly estimate either the posterior or the likelihood, and require only the ability to generate data from the simulator to analyze the model in a fully Bayesian context. 

\textit{Approximate Bayesian Computation} (ABC) constitutes a class of computational methods rooted in Bayesian statistics that can be used to evaluate posterior distributions of model parameters without having to explicitly evaluate likelihoods. At its heart, the ABC approach is quite simple. Evaluation of the likelihood is replaced by comparing synthetic data (generated by the model) to observed data, in order to assess how likely it is the model could have produced the observed data. The \textit{discrepancy} between the synthetic and observed data is compared by using \textit{summary statistics} of the data. The vanilla ABC method is built around the standard \textit{rejection sampling algorithm}, and the simulations that do not reproduce the observed data within a specified tolerance are discarded. More sophisticated ABC methods using importance sampling have also been developed, as well as additional refinements like post-processing regression adjustments. 

ABC has been successfully applied to a wide range of problems with an complex or absent associated likelihood. There are, however, several pitfalls to be aware of. The approach is simulation intensive, requires tuning of the tolerance threshold, discrepancy function and weighting function, and suffers from a curse of dimensionality of the summary statistic.  

Recently, there have been several successful studies using\textit{neural network-based conditional density estimators} to perform likelihood-free inference in simulation-based models. One such method, called \textit{Sequential Neural Posterior Estimation} (SNPE), target parametrically learning the posterior by using simulations instead of likelihood calculations. More specifically, the method trains a mixed density network (MDN) for posterior density estimation. Instead of filtering out simulations, as ABC methods do, it uses \textit{all} simulations to train the neural network to identify admissible parameters. 

 







