%================================================================
\chapter{Introduction}
%\chapter{Introduction and Objective of the Study}

%================================================================ 

The human brain contains billions of neurons, and each interact, by exchanging electrical signals, with thousands of other neurons to create countless circuits that, together with the nerves throughout our bodies, form our nervous system \cite{BrainFacts}. To understand the complex mechanisms of the nervous system, and in particular the brain's behavior, computational neuroscientists construct and analyze computational models at many different levels \cite{Sterratt}. In this thesis, we study the problem of inverse modelling, that is, the process of gathering information on a model and its parameters from measurements of what is being modelled. Inverse modelling is important because it tells us about parameters that we cannot directly observe. 

%================================================================
\section{Motivation}\label{sec:Motivation}
%================================================================

Mechanistic models in neuroscience aim to explain neural or behavioral phenomena in terms of causal mechanisms, and candidate models are validated by investigating whether proposed mechanisms can explain how experimental data manifests. The mechanistic modelling is generally through the use of differential equations, and these models often have non-measurable parameters. A central challenge in building a mechanistic model is to identify the parametrization of the system which achieves an agreement between the model and experimental data. Finding well-fitted parameters by inspection becomes more difficult as the complexity of both data and models increase, and automated identification of data-compatible parameters becomes necessary. 

Statistical inference provides the mathematical means and procedures for automated parameter identification. Statistical inference uses the \textit{likelihood function} to quantify the match between parameters and data by deriving estimators of the parameters from the data. In statistical inference, there are, broadly speaking, two paradigms for the analysis of sampled data: \textit{frequentist} inference and \textit{Bayesian} inference. In Bayesian inference, prior beliefs about parameters are updated according to \textit{Bayes' theorem} upon observing data. Bayesian inference differs from the traditional frequentist inference by the fundamental interpretation of probability. In terms of parameter inference, the frequentist view is to regard the value of some parameter as fixed but unknown, whereas the Bayesian approach to inference is to regard the parameter as a random variable having a prior probability distribution. Consequently, a posteriori knowledge will also have a probability distribution, known as the posterior distribution. This is one of the most important features of Bayesian inference, as it allows for uncertainty quantification of predictions. %One could argue that it is essential in data analysis to not only provide a good model but also an uncertainty estimate of the conclusions. 

%Many mechanistic models are defined implicitly through \textit{simulators}, i.e. a set of dynamical equations, which can be run forward to generate data. 
Many mechanistic models are defined through \textit{simulators}, which describes how the process generates data and can be run forward to generate samples from the likelihood. Likelihoods can be derived for purely statistical models, but are generally intractable or computationally infeasible for simulator models. Hence are traditional methods in the toolkit of statistical inference inaccessible for many mechanistic models. To overcome intractable likelihoods, there have been devised a suite of methods that bypass the evaluation of the likelihood function, known as \textit{simulation-based inference} (SBI), or \textit{likelihood-free inference} (LFI), methods. These methods seek to directly estimate either the posterior or the likelihood, and require only the ability to generate data from the simulator to analyze the model in a fully Bayesian context. 

\textit{Approximate Bayesian Computation} (ABC) constitutes a class of computational algorithms rooted in Bayesian statistics that can be used to evaluate posterior distributions of model parameters without having to explicitly evaluate likelihoods. At its heart, the ABC approach is quite simple; evaluation of the likelihood is replaced by comparing simulated data (generated by the simulator model) to observed data, in order to assess how likely it is that the model could have produced the observed data. The curse of dimensionality forces ABC algorithms to measure the \textit{discrepancy} between the simulated and observed data by using \textit{summary statistics} of the data. Therefore, the success of an ABC algorithm largely depends on whether or not the summary statistics capture enough information from the data that are relevant for the parameters of interest. The original ABC algorithm, proposed by Tavar√© et al. in \cite{Tavare} and later developed by Pritchard et al. in \cite{Pritchard}, is built around the standard \textit{rejection sampling algorithm}; the model parameters of simulations that do not reproduce the summary statistics of the observed data within a distance specified by a tolerance are discarded, but those that do are accepted as posterior samples. The term \textit{Approximate Bayesian Computation} was first established by Beaumont et al. in \cite{Beaumont}, who also extended the ABC approach by using Markov chain Monte Carlo (MCMC) sampling methods and post-sampling regression adjustment for correcting the posterior samples based on distances between the corresponding simulated summary statistics and the observed ones.

Recently, there have been developed simulation-based inference machine learning algorithms using conditional \textit{neural density estimators} (NDEs), that is, density estimators based on \textit{artificial neural networks} (ANNs). In particular, the  \textit{Sequential Neural Posterior Estimation} (SNPE) algorithm targets parametrically learning the posteriors over model parameters by using adaptively proposed model simulations instead of likelihood calculations. More specifically, the algorithm trains a NDE, such as a mixture-density network (MDN) or normalizing flow (NF), to learn the association between data, or summary statistics of the data, and underlying parameters. Instead of filtering out simulations, as ABC algorithms do, SNPE uses \textit{all} simulations to train the NDE to identify admissible parameters. Once trained, the network can then be applied to observed data to derive the posterior densities over the parameters of the simulator model. The strategy behind SNPE was first proposed by Papamakarios and Murray in \cite{SNL_first} and further developed to the SNPE algorithm by Lueckmann et al. in \cite{SNPE_first}. SNPE was later refined by Greenberg et al. in \cite{SNPE_apt}. In the literature, the authors refer to the variant by Papamakarios and Murray as SNPE-A, the variant by Lueckmann et al. as SNPE-B and the variant by Greenberg et al. as SNPE-C. Unless the distinction is made clear, SNPE will in the following refer to the variant by Greenberg et al. 


\subsection{Why Bayesian?} 

The frequentist approach to parameter identification yields a single best-fit. However, such a local point estimate: 
\vspace{-0.5mm}
\begin{itemize}
    \item[(i)] is potentially a poor representation of the true parameter;
    \item[(ii)] hides the fact that many similar parameter values could be capable of describing the data equally well.
\end{itemize}

The Bayesian approach to parameter identification, on the other hand, yields a probability distribution (the posterior) of all data-compatible parameters. Thus, the Bayesian approach has the advantage that the uncertainty of an estimate can be quantified due to being encoded in a probability distribution. In addition, we can characterize the model by examining the posterior over model parameters, which can inform us about the ability of the data, or, as in the case of simulation-based inference, summary statistics of the data, to constrain the model. Being able to robustly characterize the model might also aid us in designing a better model to fit the data.


