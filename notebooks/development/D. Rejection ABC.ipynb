{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "645b35d7",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# <center><font color='black'>Rejection ABC</font></center> <a class=\"tocSkip\">\n",
    "    \n",
    "### <center><font color='black'>Nicolai Haug</font></center> <a class=\"tocSkip\">\n",
    "    \n",
    "### <center><font color='black'>2021</font></center> <a class=\"tocSkip\">\n",
    "    \n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d4629d",
   "metadata": {},
   "source": [
    "# Table of Contents <a class=\"tocSkip\">\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "    * [Configure Notebook](#configure) \n",
    "* [Rejection ABC](#rejection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583e3ff3",
   "metadata": {},
   "source": [
    "# Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "\n",
    "Approximate Bayesian Computation (ABC) constitutes a class of computational methods rooted in Bayesian statistics that can be used to evaluate posterior distributions of model parameters without having to explicitly calculate likelihoods. ABC methods approximate the likelihood function by assessing how likely it is the model could have produced the observed data, based on comparing synthetic data generated by the simulator to the observed data. The simulations that do not reproduce the observed data within a specified tolerance are discarded [[1]](#references) [[2, p. ix]](#references).\n",
    "\n",
    "ABC methods have been successfully applied to a wide range of real-world problems, and have also paved the way for a range of other likelihood-free approaches. However, even though ABC methods are mathematically well-founded, they inevitably make assumptions and approximations whose impact needs to be carefully assessed [[1]](#references). In the following, we take a look at the vanilla rejection ABC algorithm and its history.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab39d626",
   "metadata": {},
   "source": [
    "## Configure Notebook <a name=\"configure\"></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: white; border: 2px solid; padding: 10px\">\n",
    "    <b><i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>&nbsp; Important</b><br>\n",
    "    <p style=\"color: black\">\n",
    "        Run the cell below to configure the notebook. \n",
    "    </p>\n",
    "<div>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98227efc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T00:53:39.542294Z",
     "start_time": "2021-03-05T00:53:38.570874Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display\n",
    "from latex_envs.latex_envs import figcaption \n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.ticker import FormatStrFormatter, LinearLocator\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import warnings\n",
    "# Comment this to turn on warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#plt.style.use('seaborn')\n",
    "sns.set()\n",
    "sns.set_context(\"paper\")\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \"0.96\"})\n",
    "\n",
    "# Set fontsizes in figures\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'axes.labelsize': 'large',\n",
    "          'axes.titlesize': 'large',\n",
    "          'xtick.labelsize': 'large',\n",
    "          'ytick.labelsize': 'large',\n",
    "          'legend.fontsize': 'large',\n",
    "          'legend.handlelength': 2}\n",
    "plt.rcParams.update(params)\n",
    "plt.rc('text', usetex=True)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47a324e",
   "metadata": {},
   "source": [
    "# Rejection ABC <a name=\"rejection\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c752bad",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "%===============================================================\n",
    "\\subsubsection{Rejection ABC}\n",
    "%===============================================================\n",
    "\n",
    "Given observed data $y_0$ and synthetic data $y$ generated by a simulator, let $\\rho (\\cdot, \\cdot)$ be a distance metric (e.g., the Euclidean norm) defined in data space $\\mathcal{R}^D$ and $\\epsilon \\geq 0$ be a tolerance. For small $\\epsilon$, the ABC approximation to the posterior is\n",
    "\n",
    "\\begin{equation}\n",
    "    p \\left(\\theta \\mid \\y = y_0 \\right) \\simeq p \\left(\\theta \\mid \\rho \\left(y, y_0 ) \\leq \\epsilon ) \n",
    "\\end{equation}\n",
    "\n",
    "Rejection ABC is a rejection-sampling method for obtaining independent samples from the approximate posterior $p \\qty(\\bm{\\theta} \\mid \\rho \\qty(\\bm{x}, \\bm{x}_0 ) \\leq \\epsilon )$. It works by first sampling a set of parameters from the prior $p(\\bm{\\theta})$, then simulating data under the model specified by the sampled parameters, and only accepting and retaining the sample if the distance between $\\bm{x}$ and $\\bm{x}_0$ is no more than $\\epsilon$. The tolerance parameter $\\epsilon$ controls the trade-off between estimation accuracy and computational efficiency. With sufficiently small $\\epsilon$, and a sensible distance metric, the accepted samples follow the exact posterior more closely, though the algorithm accepts less often. On the other hand, the algorithm accepts more often with a large $\\epsilon$, but the accepted samples will yield a replica of the prior \\cite[p. 58]{papamakarios2019neural} \\cite{abc_handbook}. \n",
    "\n",
    "An issue with ABC in general is that the required number of simulations increases dramatically as $\\epsilon$ becomes small. Moreover, likelihood-free inference also becomes challenging when the dimensionality of the data is large. A common approach to lessen this problem is to use lower-dimensional summary statistics, $S(\\bm{x})$ and $S(\\bm{x}_0)$, that capture important features such as the mean and standard deviation, in place of raw data \\cite{SNL18}. A further motivation for this approach is that real-world experiments often are interested in capturing summary statistics of the experimental data. A summary statistic that contains the same amount of information about model parameters as the whole dataset, is referred to as being a \\textit{sufficient statistic} \\cite{ABCprimer}. The acceptance criterion in the rejection ABC algorithm then becomes:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\rho \\qty(S(\\bm{x}), S(\\bm{x}_0))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48842379",
   "metadata": {},
   "source": [
    "**Algorithms** \n",
    "\n",
    "[ABC handbook]\n",
    "\n",
    "For discrete data $\\mathcal{D}$, probability model $\\mathcal{M}$ with parameters $\\theta$ having prior $\\pi(\\theta)$, we can simulate observations from: \n",
    "\n",
    "\\begin{equation}\n",
    "    \\pi (\\theta \\mid \\mathcal{D}) \\propto p(\\mathcal{D} \\mid \\theta) \\pi(\\theta),\n",
    "\\end{equation}\n",
    "\n",
    "via:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: white; border: 2px solid; padding: 10px\">\n",
    "    <b><i class=\"fa fa-calculator\" aria-hidden=\"true\"></i>&nbsp; Algorithm A</b><br>\n",
    "<div>\n",
    "    \n",
    "* __A1:__ Generate $\\theta \\sim \\pi(\\theta)$\n",
    "* __A2:__ Accept $\\theta$ with probability proportional to the likelihood $p(\\mathcal{D}\\mid \\theta)$\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "Algorithm A can be extended dramatically in its usefulness using the following, stochastically equivalent, version:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: white; border: 2px solid; padding: 10px\">\n",
    "    <b><i class=\"fa fa-calculator\" aria-hidden=\"true\"></i>&nbsp; Algorithm B (Rubin, 1984)</b><br>\n",
    "<div>\n",
    "    \n",
    "* __B1:__ Generate $\\theta \\sim \\pi(\\theta)$\n",
    "* __B2:__ Simulate an observation $\\mathcal{D}'$ from model $\\mathcal{M}$ with parameter $\\theta$\n",
    "* __B3:__ Accept $\\theta$ if $\\mathcal{D}' = \\mathcal{D}$\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "While algorithms A and B are probabilistically identical, B is much more general in that one does not need to compute probabilities explicitly to make it work; only simulation is needed. Version B is due to Rubin (1984). \n",
    "\n",
    "The drawback of B is clear. It will typically be the case that for a given value of $\\theta$, the chance of the outcome $\\mathcal{D}'=\\mathcal{D}$, namely $p(\\mathcal{D} \\mid \\theta)$, is either vanishingly small or very time consuming to compute, resulting in an algorithm that does not work effectively. This is where ABC finally comes into play, in the form of the following scheme. We start with a discrepancy metric $\\rho$ to compare datasets and a tolerance $\\epsilon \\geq 0$, and then: \n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: white; border: 2px solid; padding: 10px\">\n",
    "    <b><i class=\"fa fa-calculator\" aria-hidden=\"true\"></i>&nbsp; Algorithm C (Pritchard et al., 1999)</b><br>\n",
    "<div>\n",
    "    \n",
    "* __C1:__ Generate $\\theta \\sim \\pi(\\theta)$\n",
    "* __C2:__ Simulate an observation $\\mathcal{D}'$ from model $\\mathcal{M}$ with parameter $\\theta$\n",
    "* __C3:__ Compute $\\rho \\equiv \\rho (\\mathcal{D}', \\mathcal{D})$, and accept $\\theta$ as an appropriate draw from $\\pi (\\theta \\mid \\mathcal{D})$ if $\\rho \\leq \\epsilon$\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "The parameter $\\epsilon$ measures the tension between computability and accuracy. If $\\rho$ is a metric, then \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\rho = 0 \\quad \\implies \\quad \\mathcal{D}'=\\mathcal{D},\n",
    "\\end{equation*}\n",
    "\n",
    "so that such an accepted $\\theta$ is indeed an observation from the true posterior. \n",
    "\n",
    "Pritchard et al. (1999) were the first to describe a version of this scheme, in which the datasets in C3 were compared through a choice of summary statistics. Thus, $\\rho$ compares how well a set of simulated summary statistics matches the observed summary statistics. If the statistics are sufficient for $\\theta$, then when $\\epsilon=0$, the accepted values of $\\theta$ are still from the true posterior based on the full data. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: white; border: 2px solid; padding: 10px\">\n",
    "    <b><i class=\"fa fa-bug\" aria-hidden=\"true\"></i>&nbsp; Inquiry</b><br>\n",
    "    <p style=\"color: black\">\n",
    "        This begs the question of how one might identify 'approximately sufficient' statistics, a topic covered in <a href=\"https://github.com/nicolossus/Master-thesis/blob/master/notebooks/development/K.%20Summary%20Statistics.ipynb\">this notebook</a>.        \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e43077",
   "metadata": {},
   "source": [
    "# Rejection ABC Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ad8df04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T00:53:39.572946Z",
     "start_time": "2021-03-05T00:53:39.543955Z"
    }
   },
   "outputs": [],
   "source": [
    "def rejection_ABC(observed_data, priors, n_posterior_samples, n_sims_per_parameter, epsilon): \n",
    "    \"\"\" \n",
    "    Rejection ABC as described by the Pritchard et al. (1999) algorithm. \n",
    "    \n",
    "    This implementation expects predefined function objects: \n",
    "        1. simulator(*thetas, n_sims_per_parameter)\n",
    "        2. summary_calculator(data)\n",
    "        3. distance_metric(sim_sumstat, obs_sumstat)\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    observed_data : array\n",
    "        The observed data\n",
    "    priors : list\n",
    "        List of priors as scipy.stats objects\n",
    "    n_posterior_samples : int\n",
    "        The number of posterior samples to produce\n",
    "    n_sims_per_parameter : int\n",
    "        Image filename (with path included) of image with shape (H, W, c) to\n",
    "        transform\n",
    "    epsilon : float\n",
    "        Discrepancy threshold\n",
    "          \n",
    "    Returns\n",
    "    -------\n",
    "    samples : list \n",
    "        ABC posterior samples\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate observed data summary statistic\n",
    "    obs_sumstat = summary_calculator(observed_data)\n",
    "    \n",
    "    samples = []\n",
    "    accepted_count = 0 \n",
    "    \n",
    "    while accepted_count < n_posterior_samples: \n",
    "        # draw thetas from priors\n",
    "        thetas = [theta.rvs() for theta in priors]\n",
    "        # simulated data given realizations of drawn thetas\n",
    "        sim_data = simulator(*thetas, N)\n",
    "        # summary stat of simulated data\n",
    "        sim_sumstat = summary_calculator(sim_data)\n",
    "        # calculate discrepancy\n",
    "        distance = distance_metric(sim_sumstat, obs_sumstat)\n",
    "        # rejection step\n",
    "        if distance <= epsilon:\n",
    "            accepted_count += 1\n",
    "            samples.append(thetas)\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d01dd34",
   "metadata": {},
   "source": [
    "Most well-tested implementations will do a bit more than this under the hood, but the preceding function gives the gist of the expectation–maximization approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3fbf3c",
   "metadata": {},
   "source": [
    "# Goodness of Fit\n",
    "\n",
    "Assessing performance \n",
    "\n",
    "Expected quadratic loss\n",
    "\n",
    "https://en.wikipedia.org/wiki/Loss_function\n",
    "\n",
    "Expected loss\n",
    "\n",
    "In some contexts, the value of the loss function itself is a random quantity because it depends on the outcome of a random variable X.\n",
    "\n",
    "Both frequentist and Bayesian statistical theory involve making a decision based on the expected value of the loss function; however, this quantity is defined differently under the two paradigms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26eba3a",
   "metadata": {},
   "source": [
    "# TODOs \n",
    "\n",
    "- Sufficient vs insufficient statistics \n",
    "- $p(\\theta \\mid \\rho (\\hat{D}, D) \\leq \\epsilon)$ and $p(\\theta |D)$ as a function of $\\epsilon$. \n",
    "- The accuracy of the posterior (defined as the expected quadratic loss) delivered by ABC as a function of $\\epsilon$\n",
    "- Accuracy as a function of the number of data points in observed data\n",
    "- KL divergence\n",
    "- Accuracy vs number of posterior samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbb8ee",
   "metadata": {},
   "source": [
    "# References <a name=\"references\"></a>\n",
    "\n",
    "[1] Mikael Sunnåker et al. “Approximate Bayesian Computation”. eng. In: 9.1 (2013), e1002803. issn: 1553-734X \n",
    "\n",
    "[2] “Handbook of Approximate Bayesian Computation”. eng. 1st ed. CRC Press, 2018. isbn: 9781439881507 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb81d32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
