{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "blind-pledge",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# <center><font color='blue'>Likelihood-free Inference Methods for Parameter Identification in Mechanistic Neural Models</font></center> <a class=\"tocSkip\">\n",
    "    \n",
    "### <center><font color='blue'>Nicolai Haug</font></center> <a class=\"tocSkip\">\n",
    "    \n",
    "### <center><font color='blue'>2021</font></center> <a class=\"tocSkip\">\n",
    "    \n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-vertex",
   "metadata": {},
   "source": [
    "# Table of Contents <a class=\"tocSkip\">\n",
    "\n",
    "* [Problem Statement](#statement)\n",
    "    * [Make Contact Between Models and Experiments](#contact) \n",
    "    * [Parameter Identification](#parameter)\n",
    "    * [Models with Intractable Likelihoods](#intractable)\n",
    "    * [Likelihood-free Inference Methods](#lfi)\n",
    "        * [Approximate Bayesian Computation](#abc)\n",
    "        * [Neural Density Estimators](#nde)\n",
    "* [Objective of the Study](#objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-ecuador",
   "metadata": {},
   "source": [
    "# Problem Statement <a name=\"statement\"></a>\n",
    "\n",
    "The following introduces the fundamental problem and objective of the master project and the methodology that will be applied as well. \n",
    "\n",
    "## Make Contact Between Models and Experiments <a name=\"contact\"></a>\n",
    "\n",
    "Mechanistic models in neuroscience aim to explain neural or behavioral phenomena in terms of causal mechanisms, and candidate models are validated by investigating whether proposed mechanisms can explain how experimental data manifests. The mechanistic modelling is generally through the use of differential equations, and these models often have non-measurable parameters. A central challenge in building a mechanistic model is to identify the parametrization of the system which achieves an agreement between the model and experimental data. \n",
    "\n",
    "## Parameter Identification <a name=\"parameter\"></a>\n",
    "\n",
    "Finding well-fitted parameters by inspection becomes more difficult as the complexity of both data and models increase, and automated identification of data-compatible parameters become necessary. \n",
    "\n",
    "Statistical inference provides the mathematical means and procedures for automated parameter identification.  Statistical inference uses the **likelihood function** to quantify the match between parameters and data by deriving estimators of the parameters from the data. In statistical inference, there are, broadly speaking, two paradigms for the analysis of sampled data: **frequentist** inference and **Bayesian** inference. In Bayesian inference, posterior beliefs about parameters are updated according to **Bayes' theorem** upon observing data. Bayesian inference differs from the traditional frequentist inference by the fundamental interpretation of probability. In terms of parameter inference, the frequentist view is to regard the value of some parameter as fixed but unknown, whereas the Bayesian approach to inference is to regard the parameter as a random variable having a prior probability distribution. Consequently, one of the most important features of Bayesian inference is that it allows for uncertainty quantification of predictions. One could argue that it is essential in data analysis to not only provide a good model but also an uncertainty estimate of the conclusions. \n",
    "\n",
    "## Models with Intractable Likelihoods <a name=\"intractable\"></a>\n",
    "\n",
    "Many mechanistic models are defined implicitly through **simulators**, i.e. a set of dynamical equations and possibly a description of sources of stochasticity, which can be run forward to generate data. Likelihoods can be derived for purely statistical models, but are generally intractable or computationally infeasible for simulation-based models. Hence are traditional methods in the toolkit of statistical inference inaccessible for many mechanistic models.\n",
    "\n",
    "## Likelihood-free Inference Methods <a name=\"lfi\"></a>\n",
    "\n",
    "To overcome intractable likelihoods, a suite of methods that bypass the evaluation of the likelihood function, called **likelihood-free inference** methods, have been developed. These methods seek to directly estimate either the posterior or the likelihood, and require only the ability to generate data from the simulator to analyze the model in a fully Bayesian context. \n",
    "\n",
    "### Approximate Bayesian Computation <a name=\"abc\"></a>\n",
    "\n",
    "**Approximate Bayesian Computation (ABC)** constitutes a class of computational methods rooted in Bayesian statistics that can be used to evaluate posterior distributions of model parameters without having to explicitly evaluate likelihoods. At its heart, the ABC approach is quite simple. Evaluation of the likelihood is replaced by comparing synthetic data (generated by the model) to observed data, in order to assess how likely it is the model could have produced the observed data. The **discrepancy** between the synthetic and observed data is compared by using **summary statistics** of the data. The vanilla ABC method is built around the standard **rejection sampling algorithm**, and the simulations that do not reproduce the observed data within a specified tolerance are discarded. More sophisticated ABC methods using importance sampling have also been developed, as well as additional refinements like post-processing regression adjustments. \n",
    "\n",
    "ABC has been successfully applied to a wide range of problems with an complex or absent associated likelihood. There are, however, several pitfalls to be aware of. The approach is simulation intensive, requires tuning of the tolerance threshold, discrepancy function and weighting function, and suffers from a curse of dimensionality of the summary statistic.  \n",
    "\n",
    "### Neural Density Estimators <a name=\"nde\"></a>\n",
    "\n",
    "Recently, there have been several successful studies using **neural network-based conditional density estimators** to perform likelihood-free inference in simulation-based models. One such method, called **Sequential Neural Posterior Estimation (SNPE)**, target parametrically learning the posterior by using simulations instead of likelihood calculations. More specifically, the method trains a mixed density network (MDN) for posterior density estimation. Instead of filtering out simulations, as ABC methods do, it uses **all** simulations to train the neural network to identify admissible parameters. \n",
    "\n",
    "# Objective of the Study <a name=\"objective\"></a>\n",
    "\n",
    "Biological neural networks are complex nonlinear dynamical systems, and hence do nonlinear dynamical models play a crucial role in neuroscience as explanatory tools. Computational neuroscience has seen a rapid development over the last decades. With the advent of modern computers it is now possible to simulate large networks of neurons. \n",
    "\n",
    "In this thesis, the aim is to study the aforementioned likelihood-free methods for identifying the parametrization of mechanistic neural models that makes contact with observed data from experiments in a Bayesian context. In particular, the **Brunel network model** is of interest. The Brunel Network model exhibits a high diversity of spiking network dynamics depending on the value of only three synaptic weight parameters.\n",
    "\n",
    "One of the principal topics of research in likelihood-free inference is how to obtain state-of-the-art results with fewer simulations. By dissecting and studying the methods in great detail, another aim is to contribute to this research within the time and scope a master thesis permits.  "
   ]
  }
 ],
 "metadata": {
  "author": "Likelihood-free Inference Methods for Parameter Identification in Mechanistic Neural Models",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
